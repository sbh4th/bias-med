---
title: "The Crisis in Health Research"
subtitle: ""  
author: "Sam Harper, Nicholas B. King"
institute: " <br> </br>"
date: "<br> </br> Medicine & Society Elective  <br> 2022-04-04 </br>"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [xaringan-themer.css, style.css]
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(here)
library(DiagrammeR)
library(xaringan)
library(leaflet)
library(ggplot2)
library(emojifont)
library(countdown)
xfun::pkg_load2(c('tikzDevice', 'magick', 'pdftools'))
```

```{r, include=FALSE}
pdf2png = function(path) {
  # only do the conversion for non-LaTeX output
  if (knitr::is_latex_output()) return(path)
  path2 = xfun::with_ext(path, "png")
  img = magick::image_read_pdf(path)
  magick::image_write(img, path2, format = "png")
  path2
}
```

```{r xaringan-themer, include=FALSE}
library(xaringanthemer)
style_xaringan(text_color = "#000000", header_color = "#737373", text_font_size = "24px",  text_font_family = "'Lucida Sans'", header_font_google = google_font("Source Sans Pro"), header_font_weight="lighter", title_slide_background_color =  "#ffffff", title_slide_text_color = "#000000", link_color = "#0000ee", footnote_font_size = "0.5em")
```
.pull-left[
## Questions for you
### 1. When you read or hear about new research (publication or talk), what are things that would make you trust the result? What would make you skeptical?

### 2. Did you ever investigate it further? What did you do, and how did it go?
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "booze-90s.png"))
```
]

---
class: center, top, inverse
# .orange[**Outline**]

--

.left[
## .orange[**What is the replication crisis?**]
## .orange[**What caused the crisis?**]
## .orange[**What are some potential solutions?**]
]

---
class: center, top, inverse
# .orange[**Outline**]

.left[
## .orange[**What is the replication crisis?**]
## .gray[**What do we mean by replication?**]
## .gray[**1.3 Non-replication**]
## .gray[**1.4 Incentive structure**]
]

---
## In theory...
.footnote[See Ioannidis JPA, [2005](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)]
.pull-left[
Most studies have low prior probability of being true.

Less likely true if:
- Smaller size.
- Smaller effects.
- Many tests conducted.
- Flexible study design analysis
- Conflicts of interest present.
- Sexy/timely/popular topic.
]

.pull-right[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "jpai-plos-2005-title.png"))
```
]

---
.footnote[Source: [The Economist](https://www.economist.com/science-and-technology/2005/09/01/and-statistics)]
.left-column[

If 10% of hypotheses are true...

AND...we have 80% power...

AND...we only publish 'significant' (*p*<0.05) findings...

...What is the chance that a published positive finding is actually true? 
]

.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "economist.png"))
```
]]

---
.footnote[Source: [The Economist](https://www.economist.com/science-and-technology/2005/09/01/and-statistics)]
.left-column[

If 10% of hypotheses are true...

AND...we have 80% power...

AND...we only publish 'significant' (*p*<0.05) findings...

...What is the chance that a published positive finding is actually true? 
]

.right-column[
.center[
```{r, echo=F, out.width="100%"}
knitr::include_graphics(here("images", "jpai-plos-table.png"))
```
.white[x]  
.red[Suggests that many published results may be false.]
]]

---
.left-column[
### 2011: Early doubts about ESP in the Psych Lab
```{r, echo=F}
knitr::include_graphics(here("images", "esp.png"))
```
]

.right-column[
```{r, echo=F}
knitr::include_graphics(here("images", "bem.png"))
```
- Bem (2011) claimed to have found evidence for ESP.
- Two studies that attempted to replicate the Bem study were rejected without peer review at the same journal.

>“This journal does not publish replication studies, whether successful
or unsuccessful...I certainly agree that it’s desirable that replications
are published. The question is where. There are hundreds of journals
in psychology...We don’t want to be the Journal of Bem Replication.”

> -JPSP editor Eliot Smith, as told to the New Scientist.
]

---
.footnote[Source: Science, [2012](https://www.science.org/doi/10.1126/science.338.6112.1270)]

.pull-left[
### 2011: Fraud!
- Psychologist Diederik Stapel admits to fabricating data for many psychology studies.

- 58 papers retracted.

- Concerns raised about reliability of evidence for the entire field of psychology.
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "stapel-science.png"))
```
]

---
.footnote[Source: Pashler and Wagenmakers [2012](https://journals.sagepub.com/doi/pdf/10.1177/1745691612465253)]

.pull-left[
### Systemic concerns
- Fraud and ESP studies

- Psychologists unwilling to share data or code.

- Admission of Questionable Research Practices (QRPs).

- Evidence of manipulating results to get *p*<0.05.

- Well-known results unable to be replicated.
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "pashler-psychsci-2012.png"))
```
]

---
## Definitions
.footnote[Source: https://the-turing-way.netlify.app/reproducible-research/overview/overview-definitions.html]

.left-column[
- May not be used consistently.

- Clarify whether data and analysis are different.
]

.right-column[
```{r, echo=FALSE, out.width="80%"}
knitr::include_graphics(here("images", "rr-matrix.png"))
```
]

---
## What is a replication?
.footnote[See Nosek and Errington, PLoS Biology [2020](https://www.science.org/doi/10.1126/science.338.6112.1270). GIF from [tenor.com](https://tenor.com/view/spider-man-we-one-gif-18212100.gif).]

.pull-left[
- *Exact* replication is impossible (without time travel)

- New studies involve new subjects, different environment.

- Should not expect exactly the same results.

- Better to think of it as a study that can tell us something about the veracity of a prior claim.
]

.pull-right[
.center[
![](https://tenor.com/view/spider-man-we-one-gif-18212100.gif)]]

---
## Do we really mean 'replication'?

.center[
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "childers-title.png"))
```
]

---
## Do we really mean 'replication'?

.center[
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "childers-title-rev.png"))
```
]


---
### Considering 'robustness'

.footnote[Shanil Ebrahim, et al., “Reanalyses of randomized clinical trial data”, *JAMA*, 312(10), 2014, pp. 1024–32.]

.left-column[
- What if we tweak prior results?

- Many decisions affect the 'final' result.

- Would it matter?
]

.right-column[
.center[
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "ebrahim-title.png"))
```
]]

---
### What leads to different answers?
- Reducing measurement error:
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "ebrahim1.png"))
```
]
- Allowing for effect modification:
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "ebrahim2.png"))
```
]




---
## Potential sources of "bias" in published research
.pull-left[
### Usual explanations
#### Confounding, measurement error, selection bias, model misspecification, etc.
]

--

.pull-right[
### Problems with integrity
#### • Fraud/data manipulation/fabrication.
#### • Poor design / inadequate power.
#### • NHST: Publication bias.
#### • NHST: P-hacking.
#### • Financial ties/ideological commitments.
#### • Careerism.
#### • Lack of transparency.
]

---
.footnote[ Munafo et al. (2017)]

.center[Affects the entire research lifecycle
```{r,  echo=F, out.width = 900}
knitr::include_graphics(here("images", "munafo-figure.png"))
```
]

---
class: center, top, inverse
# .background[x]

# .orange[**How do we know that science isn't working?**]

--

# .orange[**Ask scientists.**]

---
.footnote[ Christensen et al. (2019) surveyed 3247 US researchers funded by NIH]
.left-column[
.right[
### Norm support:
### "In theory"
#### .white[x]
### "Me"
####.white[x]
### .blue["Others"]
]]

.right-column[
```{r, echo=F}
knitr::include_graphics(here("images", "christensen-norms-arrow.png"))
```
]

---
.footnote[Fanelli *PLoS ONE* (2011)]
.left-column[
### Scientists admit to engaging in questionable research practices.


]
.right-column[
.center[
```{r, echo=F, out.width="70%"}
knitr::include_graphics(here("images", "fanelli-qrp-figure.png"))
```
]]

---
.footnote[Baker *Nature* (2018)]
.left-column[
## Scentists think there is a "reproducibility" crisis

### or a "slight" crisis?  `r emo::ji("thinking_face")`
]
.right-column[
.center[
```{r, echo=F, out.width="70%"}
knitr::include_graphics(here("images", "baker-crisis.png"))
```
]]

---
class: center, top, inverse
# .orange[**1. Scientific Integrity Problems**]

.left[
## .gray[**1.1 Mertonian norms**]
## .orange[**1.2 Significance testing**]
## .gray[**1.3 Non-replication**]
## .gray[**1.4 Incentive structure**]
]
---
.pull-left[
## A lot of irreproducible or unreliable research stems from Null Hypothesis Significance Testing (NHST).
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "pushing-for-p.png"))
```
]

.footnote[ https://mobile.twitter.com/wviechtb/status/1228327958810648576/photo/1]

---
### Researcher "degrees of freedom" are difficult to control
.footnote[ Source: Gary King]

.pull-left[
### How are analyses conducted?
- collect the data over many months.
- finish recording and merging.
- run *one* regression.
- new regression, different controls.
- now a different functional form.
- new regression, different measures.
- yet another regression on subset.
- have 100 or 1000 estimates.
- 1 or maybe 5 results in the paper.
]

--

.pull-right[
### What's the problem?
- Some result is designated as the “correct” one, only *after* looking
at the estimates.

- Is this a true test of a hypothesis or just confirmation bias?

- This is "p-hacking"
]

---
```{r, echo=F}
knitr::include_graphics(here("images", "truth-vigilantes-soccer-calls2.png"))
```

.footnote[ Source: [fivethirtyeight.com](https://projects.fivethirtyeight.com/p-hacking/)]

---
# Let's do some hacking!

## Go to https://projects.fivethirtyeight.com/p-hacking/ and answer this question:

--

.center[
# .orange[**Will the 2022 US midterm elections affect the economy?**]
]

```{r, echo=F}
countdown(minutes = 3)
```

---
background-image: url(images/hacking-result.png)
background-size: contain

---
background-image: url(images/hacking-result2.png)
background-size: contain

---
## How NHST facilitates non-replication
.footnote[ Lash (2017)]
.left-column[

.red[Study results are sampled from the (---) distribution, but we only see 'statistically significant' ones ]

]
.right-column[
.center[
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "lash-aje-2017d.png"))
```
]]


---
.left-column[
.footnote[ https://www.ahajournals.org/doi/abs/10.1161/jaha.116.004880]
### How do we know there is p-hacking?
(1) Look at what people are doing.
]

.right-column[
Two estimates:
- HR=0.90, 95%CI: 0.81, 0.99    .blue["Significantly lower"]
- HR=0.89, 95%CI: 0.78, 1.00009 .red["No difference"]

```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "aha-ns.jpeg"))
```
]
---
.left-column[
.footnote[ Chavalarais et al. (2013)]
### How do we know there is p-hacking?
(2) Seriously, everything is significant
]

.right-column[
P-values in the biomedical literature, 1990-2015
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "chavalarais-fig3.png"))
```
]

---
.left-column[
.footnote[ Gotzsche (2006)]
### How do we know there is p-hacking?
(3) Maldistribution of published p-values

True for medicine, economics, psychology, political science, many other disciplines.
]

.right-column[
P-values from 260 RCTs
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "gotzsche.png"))
```
]

---
.left-column[
.footnote[ data from Barnett and Wren [(2019)](https://bmjopen.bmj.com/content/bmjopen/9/11/e032506.full.pdf)]
### Won't 95% confidence intervals help?
No.

Researchers still dichotomize them.
]

.right-column[
.center[Nearly 1,000,000 95% CIs from PubMed:]
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "wren-cis.png"))
```
]

---
class: middle, center
# NHST also leads to missing evidence and publication bias

---
.footnote[ Turner et al. *NEJM* [(2008)](https://www.nejm.org/doi/full/10.1056/nejmsa065779)]

.left-column[
###  Missing evidence
Negative studies of antidepressents less likely to be published. 

Impacts regulatory decisions.
]

.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "turner-nejm-title.png"))
```
```{r, echo=F, out.width="50%"}
knitr::include_graphics(here("images", "turner-nejm-figure.png"))
```
]]

---
.footnote[Fanelli *PLoS ONE* (2010), Yong *Nature* [(2012)](https://www.nature.com/news/replication-studies-bad-copy-1.10634)]

.pull-left[
## .orange[Publication bias affects nearly all disciplines]
### Statistically significant results are more likely to be published, across virtually all disciplines.
### May be worse in "softer" sciences.
### Much of the bias is likely self-imposed.
]

.pull-right[
.center[
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "yong-accentuate.png"))
```
]]

---
.footnote[ Figure from Mervis in Science 29 Aug 2014;345:992]

.left-column[
### Self-imposed by many researchers
221 survey experiments funded by US NSF.

All peer reviewed, required to be deposited in a registry.

All studies had results.
]

--

.right-column[
.center[
```{r, echo=F, out.width="65%"}
knitr::include_graphics(here("images", "mervis-franco-fig.png"))
```
]]

---
class: center, top, inverse
# .orange[**1. Scientific Integrity Problems**]

.left[
## .gray[**1.1 Mertonian norms**]
## .gray[**1.2 Significance testing**]
## .orange[**1.3 Non-replication**]
## .gray[**1.4 Incentive structure**]
]

---
## Distinctions between commonly used terms
.footnote[ National Academy of Sciences (2019)]
.pull-left[
### Replication
Using using independent investigators, methods, data, equipment, and protocols, we arrive at the same conclusions and/or the same estimate of the effect.

.blue[There can be good reasons why findings do not replicate.]

]

--

.pull-right[
### Reproducibility
If we start from the *same* data gathered by the scientist we can reproduce the same results, p-values, confidence intervals, tables and figures as in the original report.

.red[There are fewer reasons for non-reproducibility.]

]
---
### Large scale efforts to replicate studies are not reassuring
.footnote[ Nosek et al. (2017), Camerer et al. (2016)]

.pull-left[
#### In Psychology
```{r,  echo=F, out.width = 650}
knitr::include_graphics(here("images", "nosek-abstract.png")) 
```
]

.pull-right[
#### In Economics
```{r,  echo=F, out.width = 650}
knitr::include_graphics(here("images", "camerer-abstract.png")) 
```
]

---
.footnote[ Nosek et al. (2017)]

.left-column[
### Effect sizes are much lower in replication studies.
]
.right-column[
```{r,  echo=F, out.width = 650}
knitr::include_graphics(here("images", "nosek-fig.png")) 
```
]

---
## Surely the "top" journals are better, right?
.footnote[ Camerer et al. [(2018)](https://www.nature.com/articles/s41562-018-0399-z)]

.pull-left[
"We find a significant effect in the same direction as the original study for 13 (62%) studies, and the effect size of the replications is on average about 50% of the original effect size"

"The relative effect size of true positives is estimated to be 71%, suggesting that both .red[false positives and inflated effect sizes] of true positives contribute to imperfect reproducibility."
]
.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "camerer-nature-fig-a.png"))
```
]

---
# What about peer review?
.pull-left[
### Peer review is:
- Slow, inefficient, and expensive.

- Reviewers agreement no better than chance.

- Does not detect errors.
]

.pull-right[
### Reviewiers are biased against:
- Less prestigious institutions.

- Against new or original ideas.
]

.footnote[ Smith (2010), editor at *BMJ* for many years.]


---
### If we wanted to reproduce, often the materials aren't there
.center[
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "mol-brain.png"))
```
]

.footnote[ Miyakawa *Molecular Brain* (2020) 13:24]

---
.pull-left[
### Even with data, efforts to reproduce are </br> rarely successful
Gertler et al. gathered replication materials from published papers in econ.

Most authors only included estimation code.

*Estimation code* only ran in 40% of cases.


]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "gertler-replication.jpg"))
```
]

.footnote[ Gertler et al. 2018]

---
class: center, top, inverse
# .orange[**Outline**]


.left[
## .gray[**What is the replication crisis?**]
## .gray[**What do we mean by replication?**]
## .orange[**What are some potential solutions?**]
]

---
# What is study preregistration?
.left-column[
### A detailed study proposal that is:
]

.right-column[
### Time stamped
#### Records and publicizes time and date.


### Read-only
#### Can't be modified.

### Registered prior to data collection/access
#### Robust to fieldwork, data snooping.
]

---
# What is preregistration?
.left-column[
```{r, echo=F, out.height="100%", out.width="100%"}
knitr::include_graphics(here("images", "preregistered_large_color.png"))
# ggplot() + geom_fontawesome("fa-pencil-square-o", color='steelblue') + theme_void()
```
]

.right-column[
Common / required for publishing most RCTs

Controversial for observational studies.

Idea is to help *reduce publication bias*, since registered studies may be followed over time.

No guarantee anyone will publish.

Also can provide intellectual provenance of your ideas and hypotheses.

Good for planning and hypothesizing, .red[not a straightjacket.]
]

---
# Why preregistration?
.right-column[
## 1. It's *not* about minimizing Type 1 errors.

## 2. It *is* about:
 ###  .blue[Allowing others to transparently evaluate the credibility of the analysis.]
 ###  .blue[Assuring that all of the evidence is available for synthesis.]
]


---
# Writing up pre-registered studies

## 1. Include a link to the registration 
## 2. Report *all* pre-registered results.
## 3. Explain and justify deviations.
## 4. Non-registered analyses appropriately described as "exploratory" or "hypothesis generating".

---
# Why does preregistration matter?

## .orange[Evidence synthesis should be on *all* the evidence.]
## .orange[Distorts planning of future studies.]
## .orange[Unethical and wasteful.]

---
class: center, top, inverse
# .background[**X**]
# .orange[**Registration is useful**]

---
.footnote[redrawn from Kaplan and Irwin [(2015)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0132382)]

.left-column[
In 2000 NHLBI required the registration of primary outcome on ClinicalTrials.gov for all their grant-funded activity.
]

.right-column[
```{r, echo=F, message=F}
knitr::include_graphics(here("images", "kaplan-redrawn.png"))
```
]


---
## Emphasis on design: .red[Registered Reports]

```{r, echo=F}
knitr::include_graphics(here("images", "lee-rr-stage1.png"))
```

.footnote[ Lee (2019)]

---
## Emphasis on design: .red[Registered Reports]

```{r, echo=F}
knitr::include_graphics(here("images", "lee-rr.png"))
```

.footnote[ Lee (2019)]

---
.footnote[Allen & Mehler, *PLoS Biology* [(2019)](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000246)]

.left-column[
### RRs in Psychology

Little difference between 'replication' studies and 'novel' studies.

Big difference from non-registered studies.
]


.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "allen-rr-2019.png"))
```
]]

---
class: center, top, inverse
# .background[**X**]
# .orange[**Registration is useful**]
# .orange[**but not sufficient**]

---
.footnote[Zarin *NEJM* (2019)]
.center[
A majority of registerd RCTs still not reported.
```{r, echo=F, out.width="70%"}
knitr::include_graphics(here("images", "zarin-nejm-2019-fig.png"))
```
]


---
.footnote[ Goldacre [(2019)](https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-019-3173-2)]

.left-column[
### But is preregistration enough?

- Still many differences between registration and published reports.
]

.right-column[
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "goldacre-compare-title.png"))
```
]

---
.footnote[ Goldacre [(2019)](https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-019-3173-2)]

# Academic journals are not helping
.center[
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "goldacre-compare-table.png"))
```
]

---
# Preregistration is not a panacea

.right-column[
### Preregistered $\neq$ correct/sensible/useful
#### Transparency helps, but cannot fix terrible design or methods.

### Post-hoc analysis can be worthwhile
#### Probing surprising results or mechanisms generates knowledge.

### May also lead to 'halo' effects
#### Preregistered research deserves equal opportunity interrogation.
]

.footnote[See Nosek et al. [(2019)](https://doi.org/10.1016/j.tics.2019.07.009) and Szollosi et al. [(2020)](https://doi.org/10.1016/j.tics.2019.11.009)]

---
class: center, top, inverse
# .orange[**2. Design Solutions**]

.left[
## .gray[**2.1 Preregistration**]
## .orange[**2.2 Pre-analysis plans**]
## .gray[**2.3 Reporting guidelines**]
]

---
## **H**ypothesizing **A**fter the **R**esults are **K**nown (HARKing)

.left-column[
- Pretending what you found was what you were looking for.

- Easy to "find" theory / biological evidence consistent with results.
]

.right-column[
```{r, echo=F}
knitr::include_graphics(here("images", "targets.png"))
```
]
.footnote[ *New Yorker*, 2014-12-07]

---
# What is a pre-analysis plan?
.left-column[
```{r, echo=F}
knitr::include_graphics(here("images", "notebook.png"))
```
]

.right-column[
- Detailed description of research design and data analysis plans, submitted to a registry before looking at the data.

- Helps to tie your hands for data analysis (address researcher degrees of freedom, etc.).

- Distinguish between confirmatory and exploratory analysis.

- Increases the credibility of research.

- Transparent methods make it easier for others to build on your work.
]

---
## Confirmatory and exploratory studies have different aims
.footnote[ Dehaven and Bowman OSF (2020)]
.pull-left[
### Confirmatory
- Well-theorized.

- Plausible mechanisms.

- Minimize false positives.

- Hypothesis *testing*.
]

.pull-right[
### Exploratory
- Pushes new ideas.

- Hypothesis *generating*

- Minimize false negatives.

- Testing irrelevant.
]

---
## What goes into a pre-analysis plan?

.pull-left[
- General info (Title, PIs, Staff)

- Introduction and Summary

- Study Design:
  - Hypotheses
  - Main variables
  - Study setting.
  - Intervention components.
  - Data collection methods.
  - Treatment assignment mechanism.
  - Power calculations.
]

.pull-right[
- Analytic decisions
  - models
  - derived variables
  - clustering
  - multiple testing
  
- Threats/mitigation/robustness checks.

- Dissemination plans
]

---
# Example from development economics
.footnote[ Casey et al. [(2012)](https://academic.oup.com/qje/article-abstract/127/4/1755/1841616?redirectedFrom=fulltext)]

.pull-left[
```{r, echo=F}
knitr::include_graphics(here("images", "casey-title.png"))
```
]

--

.pull-right[
### Conclusions:
```{r, echo=F}
knitr::include_graphics(here("images", "casey-conclusion.png"))
```
]

---
## Example from epidemiology
Note the time-stamp, which provides credible evidence of *when* you had your brilliant ideas.

```{r, echo=F}
knitr::include_graphics(here("images", "harper-pap-stamp.png"))
```


---
.left-column[
### Example from epidemiology
Can be challenging for observational studies or secondary data analyses.

Can you prove when you obtained data access?
]

.right-column[
.center[
```{r, echo=F}
knitr::include_graphics(here("images", "harper-pap-abstract.png"))
```
]]

---
class: center, top, inverse
# .orange[**2. Design Solutions**]

.left[
## .gray[**2.1 Preregistration**]
## .gray[**2.2 Pre-analysis plans**]
## .orange[**2.3 Reporting guidelines**]
]

---
.footnote[ Glasziou et al. (2014)]
.pull-left[
# "Most publications have elements that are missing, poorly reported, or ambiguous"
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "glasziou-title.png"))
knitr::include_graphics(here("images", "glasziou-table.png"))
```
]

---
## Importance of intervention details
.footnote[Hoffman (2013)]
.left-column[
Want decision-makers to act on your evidence?

Can they actually understand what you did?
]

--

.right-column[
```{r, echo=F}
knitr::include_graphics(here("images", "hoffman-title.png"))
```
- Of 137 interventions, only 53 (39%) were adequately described;

- The most frequently missing item was the “intervention materials”
(47% complete);

]

---
.footnote[Hoffman (2014)]
.left-column[
Missing due to:
- copyright or intellectual property;

- absent materials or intervention details;

- unaware of their importance.
]

.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "hoffman-fig.png"))
```
]]

---
Reporting guidelines exist for entire research lifecycle

.footnote[See the [EQUATOR Network](https://www.equator-network.org/)]

.pull-left[
.right[
### Question and approach
#### .white[x]
### Pre-intervention
#### .white[x]
### Research report
#### .white[x]
### Cost-effectivness
]]

--

.pull-right[
.left[
### Systematic review
#### `r emo::ji("point_right")` PRISMA/PROSPERO
### Research protocol/preanalysis
#### `r emo::ji("point_right")` SPIRIT
### Trials/Observational studies
#### `r emo::ji("point_right")` CONSORT/STROBE
### Benefits and costs of interventions
#### `r emo::ji("point_right")` CHEERS
]]

---
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "equator-network-2020.png"))
```

---
## (Some) evidence that it might matter.
.footnote[Hopewell et al. *BMJ* [(2010)](https://www.bmj.com/content/bmj/340/bmj.c723.full.pdf)]

.left-column[
- Some evidence that item reporting has increased.

- Consistent with revised CONSORT (2001).

- Non-adopting journals report fewer items.
]

.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "hopewell-title.png"))
knitr::include_graphics(here("images", "hopewell-bmj-fig2.png"))
```
]]

---
Since 2015 funders, journals are embracing *Transparency and Openness* (TOP) guidelines.
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "TOP_standards.png"))
```
]

.footnote[Source: OSF]

---
## It's still difficult to change norms
.footnote[Hamra, Goldstein, Harper [(2019)](https://www.ahajournals.org/doi/10.1161/JAHA.119.012292)]

.pull-left[
Most journals chose *Level 1* (disclosure)

*J Am Heart Assoc* published 40 original research papers during first half of 2019.
- Posted data: 0
- Posted code: 1
- Data upon "reasonable" request: 30
- Code upon "reasonable" request: 5


]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "hamra-paper.png"))
```
]

---
# Value of reporting guidelines

.left-column[
```{r, echo=F, out.height="100%", out.width="100%"}
ggplot() + geom_fontawesome("fa-check", color='steelblue') + theme_void()
```
]

.right-column[
### Improve transparency of reported research
#### Benefits funders, producers and consumers of research.

### May help to improve the quality of research.
#### More evidence needed, unintented consequences possible.

### Better reporting $\neq$ more reliable.
#### Tranparently reported research can still be biased/bad.
]

---
class: center, middle, inverse
# .orange[**Registration, pre-analysis plans, and reporting guides are design strategies to help mitigate bias from underreported research**]

---
class: center, middle, inverse

## .orange[**They do not guarantee reliable or valid research**]

---

---
class: center, top, inverse
# .orange[**1. Scientific Integrity Problems**]

.left[
## .gray[**1.1 Mertonian norms**]
## .gray[**1.2 Significance testing**]
## .gray[**1.3 Non-replication**]
## .orange[**1.4 Incentive structure**]
]

---
## Incentive problems
.right-column[
### Reward structure
#### Papers, grants, media, "novel" and "significant" results.
### Incentives
#### Gift authorship, CV padding, salami-slicing
#### Overstating claims, ignoring "non-significant" results, p-hacking
#### Hoarding data, non-transparent materials and methods
]

---
## Incentive problems
### Remember Brian Wansink?
### After encouraging his postdoc to "find" specific results, fish for interactions, change the dependent variable, and eliminate outliers, he concluded:

.center[
```{r, echo=F}
knitr::include_graphics(here("images", "wansink-careerism.png"))
```
]

---
# Summary points
## Science is conducted by humans.
## Many counternorms exist that undermine scientific integrity.

--

.center[
## .orange[What can we do about it?]
]

---
### A reproducible path forward: Reminaging the research lifecycle?
```{r, echo=F}
knitr::include_graphics(here("images", "research-lifecycle-solution.png"))
```

.footnote[ Policy Design & Evaluation Lab (2017)]

---
class: center, top, inverse
# .orange[**Break!**] `r emo::ji("coffee")`
```{r, echo=F}
countdown(minutes = 10, 
          left = 0, right = 0, bottom = "15%", top = "15%",
          padding = "50px",
          margin = "5%",
          font_size = "7em",
          color_text= '#f5bc6c')
```