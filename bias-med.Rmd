---
title: "The Crisis in Health Research"
subtitle: ""  
author: "Sam Harper, Nicholas B. King"
institute: " <br> </br>"
date: "<br> </br> Medicine & Society Elective  <br> 2022-04-04 </br>"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [xaringan-themer.css, style.css]
    nature:
      beforeInit: "macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(here)
library(DiagrammeR)
library(xaringan)
library(leaflet)
library(ggplot2)
library(emojifont)
library(countdown)
xfun::pkg_load2(c('tikzDevice', 'magick', 'pdftools'))
```

```{r, include=FALSE}
pdf2png = function(path) {
  # only do the conversion for non-LaTeX output
  if (knitr::is_latex_output()) return(path)
  path2 = xfun::with_ext(path, "png")
  img = magick::image_read_pdf(path)
  magick::image_write(img, path2, format = "png")
  path2
}
```

```{r xaringan-themer, include=FALSE}
library(xaringanthemer)
style_xaringan(text_color = "#000000", header_color = "#737373", text_font_size = "24px",  text_font_family = "'Lucida Sans'", header_font_google = google_font("Source Sans Pro"), header_font_weight="lighter", title_slide_background_color =  "#ffffff", title_slide_text_color = "#000000", link_color = "#0000ee", footnote_font_size = "0.5em")
```

.pull-left[
```{r, echo=F}
knitr::include_graphics(here("images", "booze-90s.png"))
```
]

--

.pull-right[
## Questions for you
### 1. When you read or hear about new research (publication or talk), what are things that would make you trust the result? What would make you skeptical?

### 2. Did you ever investigate it further? What did you do, and how did it go?
]

---
class: center, top, inverse
# .orange[**Outline**]

.left[
## .orange[**What is the replication crisis?**]
## .orange[**What caused the crisis?**]
## .orange[**What are some potential solutions?**]
]

---
class: center, top, inverse
# .orange[**Outline**]

.left[
## .orange[**What is the replication crisis?**]
## .gray[**What caused the crisis?**]
## .gray[**What are some potential solutions?**]

]

---
.footnote[Source: Borgman (1997)]
.center[
```{r, echo=F}
knitr::include_graphics(here("images", "random.png"))
```
]

---
## In theory...
.footnote[See Ioannidis JPA, [2005](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)]
.pull-left[
Most studies have low prior probability of being true.

Published research is less likely true if:
- Smaller size study.
- Smaller "true" effects.
- Many tests conducted.
- Flexible study design analysis.
- Conflicts of interest present.
- Sexy/timely/popular topic.
]

.pull-right[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "jpai-plos-2005-title.png"))
```
]

---
.footnote[Source: [The Economist](https://www.economist.com/science-and-technology/2005/09/01/and-statistics)]
.left-column[

If 10% of hypotheses are true...

AND...we have 80% power...

AND...we only publish 'significant' (*p*<0.05) findings...

...What is the chance that a published positive finding is actually true? 
]

.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "economist.png"))
```
]]

---
.footnote[Source: [The Economist](https://www.economist.com/science-and-technology/2005/09/01/and-statistics)]
.left-column[

If 10% of hypotheses are true...

AND...we have 80% power...

AND...we only publish 'significant' (*p*<0.05) findings...

...What is the chance that a published positive finding is actually true? 
]

.right-column[
.center[
```{r, echo=F, out.width="100%"}
knitr::include_graphics(here("images", "jpai-plos-table.png"))
```
.white[x]  
.red[Suggests that many published results may be false.]
]]

---
.left-column[
### 2011: Early doubts about ESP in the Psych Lab
```{r, echo=F}
knitr::include_graphics(here("images", "esp.png"))
```
]

.right-column[
```{r, echo=F}
knitr::include_graphics(here("images", "bem.png"))
```
- Bem (2011) claimed to have found evidence for ESP.
- Two studies that attempted to replicate the Bem study were rejected without peer review at the same journal.

>“This journal does not publish replication studies, whether successful
or unsuccessful...I certainly agree that it’s desirable that replications
are published. The question is where. There are hundreds of journals
in psychology...We don’t want to be the Journal of Bem Replication.”

> -JPSP editor Eliot Smith, as told to the New Scientist.
]

---
.footnote[Source: Science, [2012](https://www.science.org/doi/10.1126/science.338.6112.1270)]

.pull-left[
### 2011: Fraud!
- Psychologist Diederik Stapel admits to fabricating data for many psychology studies.

- 58 papers retracted.

- Concerns raised about reliability of evidence for the entire field of psychology.
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "stapel-science.png"))
```
]

---
.footnote[Source: Pashler and Wagenmakers [2012](https://journals.sagepub.com/doi/pdf/10.1177/1745691612465253)]

.pull-left[
### Systemic concerns
- Fraud and ESP studies

- Psychologists unwilling to share data or code.

- Admission of Questionable Research Practices (QRPs).

- Evidence of manipulating results to get *p*<0.05.

- Well-known results unable to be replicated.
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "pashler-psychsci-2012.png"))
```
]

---
### Meanwhile, in biomedical research...
.footnote[Source: C Glenn Begley and Lee M Ellis, “Drug development: Raise standards for preclinical cancer research”, *Nature*, 483(7391), 2012, pp. 531–3.]

.pull-left[
Authors tried to replicate 53 “landmark” drug development studies.

Only 6 (11%) successfully replicated. 

Why not?  
- Non blinding of experimental vs. control groups. 

- Selective reporting of research results. 
]

.pull-right[
.center[
```{r, echo=F}
knitr::include_graphics(here("images", "nature-rep.png"))
```
]]

---
### Non-replication of candidate gene studies
.footnote[Ioannidis (2011)]
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "ioannidis-epid.png"))
```
]

---
.footnote[See Ritchie, *Science Fictions* (2019), Baker *Nature* (2018)]
.pull-left[
## Et cetera, et cetera...
### Also evidence of high-profile non-replication studies in:
- Psychiatry
- Neurobiology
- Chemistry
- Biology
- Ecology & evolution
- Oceanography
- Likely any place we have not looked...
]

.pull-right[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "baker-cover.png"))
```
]]

---
class: center, top, inverse
# .background[x]

# .orange[Concerns about reliablility of findings across multiple disciplines led to collaborative, large-scale *replication* studies.]

---
## Distinctions between commonly used terms
.footnote[ National Academy of Sciences (2019)]
.pull-left[
### Replication
Using using independent investigators, methods, data, equipment, and protocols, we arrive at the same conclusions and/or the same estimate of the effect.

.blue[There can be good reasons why findings do not replicate.]

]

--

.pull-right[
### Reproducibility
If we start from the *same* data gathered by the scientist we can reproduce the same results, p-values, confidence intervals, tables and figures as in the original report.

.red[There are fewer reasons for non-reproducibility.]

]

---
.footnote[Source: https://the-turing-way.netlify.app/reproducible-research/overview/overview-definitions.html]

.left-column[
## Definitions
- May not be used consistently.

- Clarify whether data and analysis are different from original study.
]

.right-column[
.center[
```{r, echo=FALSE, out.width="80%"}
knitr::include_graphics(here("images", "rr-matrix.png"))
```
]]

---
.footnote[Nosek et al., [2015](https://www-science-org/doi/10.1126/science.aac4716). Also see letters and responses: http://bit.ly/1Xg9lQl]

.pull-left[
```{r, echo=F}
knitr::include_graphics(here("images", "osc-2015-method.png"))
```
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "nosek-abstract.png"))
```
]

---
.footnote[ Nosek et al. (2015)]

.left-column[
### Effect sizes are much lower in replication studies.
]
.right-column[
```{r,  echo=F, out.width = 600}
knitr::include_graphics(here("images", "nosek-fig.png")) 
```
]

---
.footnote[Camerer et al. (2016)]

.left-column[
### No better in Economics
- Increased the number of participants by a factor of five

- Preregistered their study and analysis designs before any data collection.
]

.right-column[
.center[
```{r,  echo=F, out.width = 650}
knitr::include_graphics(here("images", "camerer-abstract.png")) 
```
]]

---
## Surely the "top" journals are better, right?
.footnote[ Camerer et al. [(2018)](https://www.nature.com/articles/s41562-018-0399-z)]

.pull-left[
"We find a significant effect in the same direction as the original study for 13 (62%) studies, and the effect size of the replications is on average about 50% of the original effect size"

"The relative effect size of true positives is estimated to be 71%, suggesting that both .red[false positives and inflated effect sizes] of true positives contribute to imperfect reproducibility."
]
.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "camerer-nature-fig-a.png"))
```
]

---
## What is a replication?
.footnote[See Nosek and Errington, PLoS Biology [2020](https://www.science.org/doi/10.1126/science.338.6112.1270). GIF from [tenor.com](https://tenor.com/view/spider-man-we-one-gif-18212100.gif).]

.pull-left[
- *Exact* replication is impossible (without time travel)

- New studies involve new subjects, different environment.

- Should not expect exactly the same results.

- Better to think of it as a study that can tell us something about the veracity of a prior claim.
]

.pull-right[
.center[
![](https://tenor.com/view/spider-man-we-one-gif-18212100.gif)]]

---
.footnote[Source: Evans et al., *Testing Treatments* (2011)]
.left-column[
### Can think of cluster-randomized trials as replications
]
.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "crash2-meta-analysis.png"))
```
]]

---
## Do we really mean 'replication'?

.center[
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "childers-title.png"))
```
]

---
## Do we really mean 'replication'?

.center[
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "childers-title-rev.png"))
```
]


---
### Considering 'robustness'

.footnote[Shanil Ebrahim, et al., “Reanalyses of randomized clinical trial data”, *JAMA*, 312(10), 2014, pp. 1024–32.]

.left-column[
- What if we tweak prior methods?

- Many decisions affect the 'final' result.

- Would it matter?
]

.right-column[
.center[
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "ebrahim-title.png"))
```
]]

---
### What leads to different answers?
- Reducing measurement error:
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "ebrahim1.png"))
```
]
- Allowing for effect modification:
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "ebrahim2.png"))
```
]

---
.footnote[Baker *Nature* (2018)]
.left-column[
## Scentists think there is a "reproducibility" crisis

### or a "slight" crisis?  `r emo::ji("thinking_face")`
]
.right-column[
.center[
```{r, echo=F, out.width="70%"}
knitr::include_graphics(here("images", "baker-crisis.png"))
```
]]

---
class: center, top, inverse
# .orange[**Outline**]

.left[
## .gray[**What is the replication crisis?**]
## .orange[**What caused the crisis?**]
## .gray[**What are some potential solutions?**]
]


---
## Potential sources of "bias" in published research
.pull-left[
### Usual explanations
#### Confounding, measurement error, selection bias, model misspecification, etc.
]

--

.pull-right[
### Problems with integrity
#### • Fraud/data manipulation/fabrication.
#### • Poor design / inadequate power.
#### • NHST: Publication bias.
#### • NHST: P-hacking.
#### • Financial ties/ideological commitments.
#### • Careerism.
#### • Lack of transparency.
]

---
.footnote[ George, Int J Clin Oncol (2016) 21:15–21]

.center[
```{r,  echo=F, out.width = "100%"}
knitr::include_graphics(here("images", "george-fraud-table.png"))
```
]

---
### The coronavirus pandemic has not helped.
.footnote[See reporting by Davey et al. in [*The Guardian*](https://www.theguardian.com/world/2020/jun/03/covid-19-surgisphere-who-world-health-organization-hydroxychloroquine)]

.pull-left[
- High profile studies of 96k patients across 671 hospitals.

- Claimed hydroxychloroquine increased mortality from COVID-19.

- Immediately led to WHO halting the hydroxychloroquine arm of its global trials.

- Researchers subsequently questioned irregularities in the data, ethics review, protocols, statistical analysis. 

- Two papers (other in *NEJM*) were retracted.
]

.pull-right[
```{r,  echo=F, out.width = "100%"}
knitr::include_graphics(here("images", "surgisphere-lancet-retracted.png"))
```
]

---
### The coronavirus pandemic has not helped.
.footnote[See reporting by [Meyerowitz-Katz](https://gidmk.medium.com/is-ivermectin-for-covid-19-based-on-fraudulent-research-part-4-f30eeb30d2ff)]

.pull-left[
- Ivermectin known to be effective for parasitic diseases.

- Lebanese trial claimed *huge effect* of ivermectin on lowering SARS-CoV2 viral load.

- Requests for trial data to verify the results by independent researchers were denied.

- Researchers subsequently gained access, found serious irregularities. 

- Ultimately retracted.
]

.pull-right[
```{r,  echo=F, out.width = "100%"}
knitr::include_graphics(here("images", "ivermectin-trial-data.png"))
```
]

---
.footnote[Reis et al. *NEJM*, March 30, 2022 DOI: 10.1056/NEJMoa2115869]

.center[
```{r,  echo=F, out.width = "80%"}
knitr::include_graphics(here("images", "reis-nejm-title.png"))
knitr::include_graphics(here("images", "reis-nejm-table2.png"))
```
]

---
.pull-left[
## A lot of irreproducible or unreliable research stems from Null Hypothesis Significance Testing (NHST).
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "pushing-for-p.png"))
```
]

.footnote[ https://mobile.twitter.com/wviechtb/status/1228327958810648576/photo/1]

---
.footnote[ Lash (2017)]
.left-column[
### NHST facilitates non-replication
.red[Study results are sampled from the (---) distribution, but we only see 'statistically significant' ones ]

]
.right-column[
.center[
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "lash-aje-2017d.png"))
```
]]

--
Implications for planning, meta-analysis, and replication studies.

---
### The coronavirus pandemic has not helped.
.footnote[Source: Pearson, *Nature* (2021)]

.pull-left[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "pearson-samples.png"))
```
]]

.pull-right[
.center[
```{r, echo=F, out.width="70%"}
knitr::include_graphics(here("images", "pearson-trials.png"))
```
]]

---
### Researcher "degrees of freedom" are difficult to control
.footnote[ Source: Gary King]

.pull-left[
### How are analyses conducted?
- collect the data over many months.
- finish recording and merging.
- run *one* regression.
- new regression, different controls.
- now a different functional form.
- new regression, different measures.
- yet another regression on subset.
- have 100 or 1000 estimates.
- 1 or maybe 5 results in the paper.
]

--

.pull-right[
### What's the problem?
- Some result is designated as the “correct” one, only *after* looking
at the estimates.

- Is this a true test of a hypothesis or just confirmation bias?

- This is "p-hacking"
]

---
```{r, echo=F}
knitr::include_graphics(here("images", "truth-vigilantes-soccer-calls2.png"))
```

.footnote[ Source: [fivethirtyeight.com](https://projects.fivethirtyeight.com/p-hacking/)]

---
# Let's do some hacking!

## Go to https://projects.fivethirtyeight.com/p-hacking/ and answer this question:

--

.center[
# .orange[**Will the 2022 US midterm elections affect the economy?**]
]

```{r, echo=F}
countdown(minutes = 3)
```

---
background-image: url(images/hacking-result.png)
background-size: contain

---
background-image: url(images/hacking-result2.png)
background-size: contain


---
.left-column[
.footnote[ https://www.ahajournals.org/doi/abs/10.1161/jaha.116.004880]
### How do we know there is p-hacking?
(1) Look at what people are doing.
]

.right-column[
Two estimates:
- HR=0.90, 95%CI: 0.81, 0.99    .blue["Significantly lower"]
- HR=0.89, 95%CI: 0.78, 1.00009 .red["No difference"]

```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "aha-ns.jpeg"))
```
]

---
### Good advice on how to get *p*<0.05

.pull-left[
```{r, echo=F}
knitr::include_graphics(here("images", "wansink-manipulation.png"))
knitr::include_graphics(here("images", "wansink-p-hacking-dv.png"))
```
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "wansink-p-hacking-int.png"))
```
]

---
.footnote[Archived post [here](https://web.archive.org/web/20170312041524/http:/www.brianwansink.com/phd-advice/the-grad-student-who-never-said-no).]

.pull-left[
*"I gave her a data set of a self-funded, failed study which had .red[null results]... I said, ‘This cost us a lot of time and our own money to collect. There’s got to be something here we can salvage because it’s a cool (rich & unique) data set.’ I had three ideas for potential Plan B, C, & D directions (since Plan A had failed)."* -blog, 2016

Enterprising grad students found:
- impossible values
- incorrect ANOVA results
- dubious p-values

Wansink denied requests for access to the original data.
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "wansink-vox.png"))
```

Wansink resigned from Cornell in 2019.
]


---
.left-column[
.footnote[ Chavalarais et al. (2013)]
### How do we know there is p-hacking?
(2) Seriously, everything is significant
]

.right-column[
P-values in the biomedical literature, 1990-2015
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "chavalarais-fig3.png"))
```
]

---
.left-column[
.footnote[ Gotzsche (2006)]
### How do we know there is p-hacking?
(3) Maldistribution of published p-values

True for medicine, economics, psychology, political science, many other disciplines.
]

.right-column[
P-values from 260 RCTs
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "gotzsche.png"))
```
]

---
.left-column[
.footnote[ data from Barnett and Wren [(2019)](https://bmjopen.bmj.com/content/bmjopen/9/11/e032506.full.pdf)]
### Won't 95% confidence intervals help?
No.

Researchers still dichotomize them.
]

.right-column[
.center[Nearly 1,000,000 95% CIs from PubMed:]
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "wren-cis.png"))
```
]

---
class: middle, center
# NHST also leads to missing evidence and publication bias

---
.footnote[ Turner et al. *NEJM* [(2008)](https://www.nejm.org/doi/full/10.1056/nejmsa065779)]

.left-column[
###  Missing evidence
Negative studies of antidepressents less likely to be published. 

Impacts regulatory decisions.
]

.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "turner-nejm-title.png"))
```
```{r, echo=F, out.width="50%"}
knitr::include_graphics(here("images", "turner-nejm-figure.png"))
```
]]

---
.footnote[Fanelli *PLoS ONE* (2010), Yong *Nature* [(2012)](https://www.nature.com/news/replication-studies-bad-copy-1.10634)]

.pull-left[
## .orange[Publication bias affects nearly all disciplines]
### Statistically significant results are more likely to be published, across virtually all disciplines.
### May be worse in "softer" sciences.
### Much of the bias is likely self-imposed.
]

.pull-right[
.center[
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "yong-accentuate.png"))
```
]]

---
.footnote[ Figure from Mervis in Science 29 Aug 2014;345:992]

.left-column[
### Self-imposed by many researchers
221 survey experiments funded by US NSF.

All peer reviewed, required to be deposited in a registry.

All studies had results.
]

.right-column[
.center[
```{r, echo=F, out.width="65%"}
knitr::include_graphics(here("images", "mervis-franco-fig.png"))
```
]]

---
# What about peer review?
.pull-left[
### Peer review is:
- Slow, inefficient, and expensive.

- Reviewers agreement no better than chance.

- Does not detect errors.
]

.pull-right[
### Reviewiers are biased against:
- Less prestigious institutions.

- Against new or original ideas.
]

.footnote[ Smith (2010), editor at *BMJ* for many years.]

---
## Okay, forget peer review
.footnote[See https://www.cbc.ca/news/health/covid-19-vaccine-study-error-anti-vaxxers-1.6188806]
.pull-left[
- Study *preprints* get science out rapidly, before peer review.

- Explosion of preprints during COVID-19

- 12 authors from Ottawa Heart Institute published preprint claiming 1 in 1000 vaccine recipients had myocarditis.
  - Case series
  - Wrong denominators (33,000 instead of 850,000)
  - Revised risk 1 in 25,000
]

.pull-right[
.center[
```{r, echo=F, out.width="100%"}
knitr::include_graphics(here("images", "kafil-withdrawn.png"))
```
]]


---
### If we wanted to reproduce, often the materials aren't there
.center[
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "mol-brain.png"))
```
]

.footnote[ Miyakawa *Molecular Brain* (2020) 13:24]

---
.pull-left[
### Even with data, efforts to reproduce are </br> rarely successful
Gertler et al. gathered replication materials from published papers in econ.

Most authors only included estimation code.

*Estimation code* only ran in 40% of cases.


]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "gertler-replication.jpg"))
```
]

.footnote[ Gertler et al. 2018]

---
class: center, top, inverse
# .orange[**Outline**]


.left[
## .gray[**What is the replication crisis?**]
## .gray[**What do we mean by replication?**]
## .orange[**What are some potential solutions?**]
]

---
class: center, middle, inverse

# .orange[**Preregistration of studies**]

---
# What is study preregistration?
.left-column[
### A detailed study proposal that is:
]

.right-column[
### Time stamped
#### Records and publicizes time and date.


### Read-only
#### Can't be modified.

### Registered prior to data collection/access
#### Robust to fieldwork, data snooping.
]

---
# What is preregistration?
.left-column[
```{r, echo=F, out.height="100%", out.width="100%"}
knitr::include_graphics(here("images", "preregistered_large_color.png"))
# ggplot() + geom_fontawesome("fa-pencil-square-o", color='steelblue') + theme_void()
```
]

.right-column[
Common / required for publishing most RCTs

Controversial for observational studies.

Idea is to help *reduce publication bias*, since registered studies may be followed over time.

No guarantee anyone will publish.

Also can provide intellectual provenance of your ideas and hypotheses.

Good for planning and hypothesizing, .red[not a straightjacket.]
]

---
# Why preregistration?
.right-column[
## 1. It's *not* about minimizing Type 1 errors.

## 2. It *is* about:
 ###  .blue[Allowing others to transparently evaluate the credibility of the analysis.]
 ###  .blue[Assuring that all of the evidence is available for synthesis.]
]

---
# Why does preregistration matter?

## .orange[Evidence synthesis should be on *all* the evidence.]
## .orange[Distorts planning of future studies.]
## .orange[Unethical and wasteful.]

---
class: center, top, inverse
# .background[**X**]
# .orange[**Registration is useful**]

---
.footnote[redrawn from Kaplan and Irwin [(2015)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0132382)]

.left-column[
In 2000 NHLBI required the registration of primary outcome on ClinicalTrials.gov for all their grant-funded activity.
]

.right-column[
```{r, echo=F, message=F}
knitr::include_graphics(here("images", "kaplan-redrawn.png"))
```
]

---
class: center, middle, inverse

# .orange[**What if my results are null?**]

# .orange[**You showed us that they won't get published!**]



---
## Emphasis on design: .red[Registered Reports]

```{r, echo=F}
knitr::include_graphics(here("images", "lee-rr-stage1.png"))
```

.footnote[ Lee (2019)]

---
## Emphasis on design: .red[Registered Reports]

```{r, echo=F}
knitr::include_graphics(here("images", "lee-rr.png"))
```

.footnote[ Lee (2019)]

---
.footnote[Allen & Mehler, *PLoS Biology* [(2019)](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000246)]

.left-column[
### RRs in Psychology

Little difference between 'replication' studies and 'novel' studies.

Big difference from non-registered studies.
]


.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "allen-rr-2019.png"))
```
]]

---
class: center, top, inverse
# .background[**X**]
# .orange[**Registration is useful**]
# .orange[**but not sufficient**]

---
.footnote[Zarin *NEJM* (2019)]
.center[
A majority of (pre-COVID-19) registered RCTs still not reported.
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "zarin-nejm-2019-fig.png"))
```
]


---
.footnote[ Goldacre [(2019)](https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-019-3173-2)]

.left-column[
### But is preregistration enough?

- Still many differences between registration and published reports.
]

.right-column[
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "goldacre-compare-title.png"))
```
]

---
.footnote[ Goldacre [(2019)](https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-019-3173-2)]

# Academic journals are not helping
.center[
```{r, echo=F, out.width=700}
knitr::include_graphics(here("images", "goldacre-compare-table.png"))
```
]

---
# Preregistration is not a panacea

.right-column[
### Preregistered $\neq$ correct/sensible/useful
#### Transparency helps, but cannot fix terrible design or methods.

### Post-hoc analysis can be worthwhile
#### Probing surprising results or mechanisms generates knowledge.

### May also lead to 'halo' effects
#### Preregistered research deserves equal opportunity interrogation.
]

.footnote[See Nosek et al. [(2019)](https://doi.org/10.1016/j.tics.2019.07.009) and Szollosi et al. [(2020)](https://doi.org/10.1016/j.tics.2019.11.009)]

---
class: center, middle, inverse

# .orange[**Pre-analysis plans**]


---
## **H**ypothesizing **A**fter the **R**esults are **K**nown (HARKing)
.footnote[ *New Yorker*, 2014-12-07]

.left-column[
- Pretending what you found was what you were looking for.

- Easy to "find" theory / biological evidence consistent with results.
]

.right-column[
```{r, echo=F}
knitr::include_graphics(here("images", "targets.png"))
```
]


---
# What is a pre-analysis plan?
.left-column[
```{r, echo=F}
knitr::include_graphics(here("images", "notebook.png"))
```
]

.right-column[
- Detailed description of research design and data analysis plans, submitted to a registry before looking at the data.

- Helps to tie your hands for data analysis (address researcher degrees of freedom, etc.).

- Distinguish between confirmatory and exploratory analysis.

- Increases the credibility of research.

- Transparent methods make it easier for others to build on your work.
]

---
## Confirmatory and exploratory studies have different aims
.footnote[ Dehaven and Bowman OSF (2020)]
.pull-left[
### Confirmatory
- Well-theorized.

- Plausible mechanisms.

- Minimize false positives.

- Hypothesis *testing*.
]

.pull-right[
### Exploratory
- Pushes new ideas.

- Hypothesis *generating*

- Minimize false negatives.

- Testing irrelevant.
]

---
## What goes into a pre-analysis plan?

.pull-left[
- General info (Title, PIs, Staff)

- Introduction and Summary

- Study Design:
  - Hypotheses
  - Main variables
  - Study setting.
  - Intervention components.
  - Data collection methods.
  - Treatment assignment mechanism.
  - Power calculations.
]

.pull-right[
- Analytic decisions
  - models
  - derived variables
  - clustering
  - multiple testing
  
- Threats/mitigation/robustness checks.

- Dissemination plans
]

---
## Example from epidemiology
Note the time-stamp, which provides credible evidence of *when* you had your brilliant ideas.

```{r, echo=F}
knitr::include_graphics(here("images", "harper-pap-stamp.png"))
```


---
.left-column[
### Example from epidemiology
Can be challenging for observational studies or secondary data analyses.

Can you prove when you obtained data access?
]

.right-column[
.center[
```{r, echo=F}
knitr::include_graphics(here("images", "harper-pap-abstract.png"))
```
]]

---
class: center, top, inverse
# .orange[**2. Design Solutions**]

.left[
## .gray[**2.1 Preregistration**]
## .gray[**2.2 Pre-analysis plans**]
## .orange[**2.3 Reporting guidelines**]
]

---
.footnote[ Glasziou et al. (2014)]
.pull-left[
# "Most publications have elements that are missing, poorly reported, or ambiguous"
]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "glasziou-title.png"))
knitr::include_graphics(here("images", "glasziou-table.png"))
```
]

---
## Importance of intervention details
.footnote[Hoffman (2013)]
.left-column[
Want decision-makers to act on your evidence?

Can they actually understand what you did?
]

--

.right-column[
```{r, echo=F}
knitr::include_graphics(here("images", "hoffman-title.png"))
```
- Of 137 interventions, only 53 (39%) were adequately described;

- The most frequently missing item was the “intervention materials”
(47% complete);

]

---
.footnote[Hoffman (2014)]
.left-column[
Missing due to:
- copyright or intellectual property;

- absent materials or intervention details;

- unaware of their importance.
]

.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "hoffman-fig.png"))
```
]]

---
Reporting guidelines exist for entire research lifecycle

.footnote[See the [EQUATOR Network](https://www.equator-network.org/)]

.pull-left[
.right[
### Question and approach
#### .white[x]
### Pre-intervention
#### .white[x]
### Research report
#### .white[x]
### Cost-effectivness
]]

--

.pull-right[
.left[
### Systematic review
#### `r emo::ji("point_right")` PRISMA/PROSPERO
### Research protocol/preanalysis
#### `r emo::ji("point_right")` SPIRIT
### Trials/Observational studies
#### `r emo::ji("point_right")` CONSORT/STROBE
### Benefits and costs of interventions
#### `r emo::ji("point_right")` CHEERS
]]

---
```{r, echo=F, out.width="90%"}
knitr::include_graphics(here("images", "equator-network-2020.png"))
```

---
## (Some) evidence that it might matter.
.footnote[Hopewell et al. *BMJ* [(2010)](https://www.bmj.com/content/bmj/340/bmj.c723.full.pdf)]

.left-column[
- Some evidence that item reporting has increased.

- Consistent with revised CONSORT (2001).

- Non-adopting journals report fewer items.
]

.right-column[
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "hopewell-title.png"))
knitr::include_graphics(here("images", "hopewell-bmj-fig2.png"))
```
]]

---
Since 2015 funders, journals are embracing *Transparency and Openness* (TOP) guidelines.
.center[
```{r, echo=F, out.width="80%"}
knitr::include_graphics(here("images", "TOP_standards.png"))
```
]

.footnote[Source: OSF]

---
## It's still difficult to change norms
.footnote[Hamra, Goldstein, Harper [(2019)](https://www.ahajournals.org/doi/10.1161/JAHA.119.012292)]

.pull-left[
Most journals chose *Level 1* (disclosure)

*J Am Heart Assoc* published 40 original research papers during first half of 2019.
- Posted data: 0
- Posted code: 1
- Data upon "reasonable" request: 30
- Code upon "reasonable" request: 5


]

.pull-right[
```{r, echo=F}
knitr::include_graphics(here("images", "hamra-paper.png"))
```
]

---
# Value of reporting guidelines

.left-column[
```{r, echo=F, out.height="100%", out.width="100%"}
ggplot() + geom_fontawesome("fa-check", color='steelblue') + theme_void()
```
]

.right-column[
### Improve transparency of reported research
#### Benefits funders, producers and consumers of research.

### May help to improve the quality of research.
#### More evidence needed, unintented consequences possible.

### Better reporting $\neq$ more reliable.
#### Tranparently reported research can still be biased/bad.
]

---
class: center, middle, inverse
# .orange[**Registration, pre-analysis plans, and reporting guides are design strategies to help mitigate bias from underreported research**]

---
class: center, middle, inverse

## .orange[**They do not guarantee reliable or valid research**]

---
## Incentive problems
.right-column[
### Reward structure
#### Papers, grants, media, "novel" and "significant" results.
### Incentives
#### Gift authorship, CV padding, salami-slicing
#### Overstating claims, ignoring "non-significant" results, p-hacking
#### Hoarding data, non-transparent materials and methods
]

---
# Summary points
.right-column[
### Science is conducted by humans.
#### Extra-scientific factors matter, not necessarily malicious intent. 

### Changing norms is hard
#### Many incentives exist that undermine scientific integrity.

### Transparency and openness can help
#### Making research process transparent is the bare minimum, and does not guarantee 'true' results.
]
