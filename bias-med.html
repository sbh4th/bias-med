<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>The Crisis in Health Research</title>
    <meta charset="utf-8" />
    <meta name="author" content="Sam Harper, Nicholas B. King" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# The Crisis in Health Research
### Sam Harper, Nicholas B. King
### <br> </br>
### <br> </br> Medicine &amp; Society Elective <br> 2022-04-04 </br>

---






.pull-left[
## Questions for you
### 1. When you read or hear about new research (publication or talk), what are things that would make you trust the result? What would make you skeptical?

### 2. Did you ever investigate it further? What did you do, and how did it go?
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/booze-90s.png" width="1055" /&gt;
]

---
class: center, top, inverse
# .orange[**Outline**]

--

.left[
## .orange[**What is the replication crisis?**]
## .orange[**What caused the crisis?**]
## .orange[**What are some potential solutions?**]
]

---
class: center, top, inverse
# .orange[**Outline**]

.left[
## .orange[**What is the replication crisis?**]
## .gray[**What do we mean by replication?**]
## .gray[**1.3 Non-replication**]
## .gray[**1.4 Incentive structure**]
]

---
## In theory...
.footnote[See Ioannidis JPA, [2005](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)]
.pull-left[
Most studies have low prior probability of being true.

Less likely true if:
- Smaller size.
- Smaller effects.
- Many tests conducted.
- Flexible study design analysis
- Conflicts of interest present.
- Sexy/timely/popular topic.
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/jpai-plos-2005-title.png" width="80%" /&gt;
]

---
.footnote[Source: [The Economist](https://www.economist.com/science-and-technology/2005/09/01/and-statistics)]
.left-column[

If 10% of hypotheses are true...

AND...we have 80% power...

AND...we only publish 'significant' (*p*&lt;0.05) findings...

...What is the chance that a published positive finding is actually true? 
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/economist.png" width="80%" /&gt;
]]

---
.footnote[Source: [The Economist](https://www.economist.com/science-and-technology/2005/09/01/and-statistics)]
.left-column[

If 10% of hypotheses are true...

AND...we have 80% power...

AND...we only publish 'significant' (*p*&lt;0.05) findings...

...What is the chance that a published positive finding is actually true? 
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/jpai-plos-table.png" width="100%" /&gt;
.white[x]  
.red[Suggests that many published results may be false.]
]]

---
.left-column[
### 2011: Early doubts about ESP in the Psych Lab
&lt;img src="/Users/samharper/git/bias-med/images/esp.png" width="803" /&gt;
]

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/bem.png" width="1356" /&gt;
- Bem (2011) claimed to have found evidence for ESP.
- Two studies that attempted to replicate the Bem study were rejected without peer review at the same journal.

&gt;‚ÄúThis journal does not publish replication studies, whether successful
or unsuccessful...I certainly agree that it‚Äôs desirable that replications
are published. The question is where. There are hundreds of journals
in psychology...We don‚Äôt want to be the Journal of Bem Replication.‚Äù

&gt; -JPSP editor Eliot Smith, as told to the New Scientist.
]

---
.footnote[Source: Science, [2012](https://www.science.org/doi/10.1126/science.338.6112.1270)]

.pull-left[
### 2011: Fraud!
- Psychologist Diederik Stapel admits to fabricating data for many psychology studies.

- 58 papers retracted.

- Concerns raised about reliability of evidence for the entire field of psychology.
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/stapel-science.png" width="1547" /&gt;
]

---
.footnote[Source: Pashler and Wagenmakers [2012](https://journals.sagepub.com/doi/pdf/10.1177/1745691612465253)]

.pull-left[
### Systemic concerns
- Fraud and ESP studies

- Psychologists unwilling to share data or code.

- Admission of Questionable Research Practices (QRPs).

- Evidence of manipulating results to get *p*&lt;0.05.

- Well-known results unable to be replicated.
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/pashler-psychsci-2012.png" width="1837" /&gt;
]

---
## Definitions
.footnote[Source: https://the-turing-way.netlify.app/reproducible-research/overview/overview-definitions.html]

.left-column[
- May not be used consistently.

- Clarify whether data and analysis are different.
]

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/rr-matrix.png" width="80%" /&gt;
]

---
## What is a replication?
.footnote[See Nosek and Errington, PLoS Biology [2020](https://www.science.org/doi/10.1126/science.338.6112.1270). GIF from [tenor.com](https://tenor.com/view/spider-man-we-one-gif-18212100.gif).]

.pull-left[
- *Exact* replication is impossible (without time travel)

- New studies involve new subjects, different environment.

- Should not expect exactly the same results.

- Better to think of it as a study that can tell us something about the veracity of a prior claim.
]

.pull-right[
.center[
![](https://tenor.com/view/spider-man-we-one-gif-18212100.gif)]]

---
## Do we really mean 'replication'?

.center[
&lt;img src="/Users/samharper/git/bias-med/images/childers-title.png" width="90%" /&gt;
]

---
## Do we really mean 'replication'?

.center[
&lt;img src="/Users/samharper/git/bias-med/images/childers-title-rev.png" width="90%" /&gt;
]


---
### Considering 'robustness'

.footnote[Shanil Ebrahim, et al., ‚ÄúReanalyses of randomized clinical trial data‚Äù, *JAMA*, 312(10), 2014, pp. 1024‚Äì32.]

.left-column[
- What if we tweak prior results?

- Many decisions affect the 'final' result.

- Would it matter?
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/ebrahim-title.png" width="90%" /&gt;
]]

---
### What leads to different answers?
- Reducing measurement error:
.center[
&lt;img src="/Users/samharper/git/bias-med/images/ebrahim1.png" width="80%" /&gt;
]
- Allowing for effect modification:
.center[
&lt;img src="/Users/samharper/git/bias-med/images/ebrahim2.png" width="80%" /&gt;
]




---
## Potential sources of "bias" in published research
.pull-left[
### Usual explanations
#### Confounding, measurement error, selection bias, model misspecification, etc.
]

--

.pull-right[
### Problems with integrity
#### ‚Ä¢ Fraud/data manipulation/fabrication.
#### ‚Ä¢ Poor design / inadequate power.
#### ‚Ä¢ NHST: Publication bias.
#### ‚Ä¢ NHST: P-hacking.
#### ‚Ä¢ Financial ties/ideological commitments.
#### ‚Ä¢ Careerism.
#### ‚Ä¢ Lack of transparency.
]

---
.footnote[ Munafo et al. (2017)]

.center[Affects the entire research lifecycle
&lt;img src="/Users/samharper/git/bias-med/images/munafo-figure.png" width="900" /&gt;
]

---
class: center, top, inverse
# .background[x]

# .orange[**How do we know that science isn't working?**]

--

# .orange[**Ask scientists.**]

---
.footnote[ Christensen et al. (2019) surveyed 3247 US researchers funded by NIH]
.left-column[
.right[
### Norm support:
### "In theory"
#### .white[x]
### "Me"
####.white[x]
### .blue["Others"]
]]

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/christensen-norms-arrow.png" width="1707" /&gt;
]

---
.footnote[Fanelli *PLoS ONE* (2011)]
.left-column[
### Scientists admit to engaging in questionable research practices.


]
.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/fanelli-qrp-figure.png" width="70%" /&gt;
]]

---
.footnote[Baker *Nature* (2018)]
.left-column[
## Scentists think there is a "reproducibility" crisis

### or a "slight" crisis?  ü§î
]
.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/baker-crisis.png" width="70%" /&gt;
]]

---
class: center, top, inverse
# .orange[**1. Scientific Integrity Problems**]

.left[
## .gray[**1.1 Mertonian norms**]
## .orange[**1.2 Significance testing**]
## .gray[**1.3 Non-replication**]
## .gray[**1.4 Incentive structure**]
]
---
.pull-left[
## A lot of irreproducible or unreliable research stems from Null Hypothesis Significance Testing (NHST).
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/pushing-for-p.png" width="2435" /&gt;
]

.footnote[ https://mobile.twitter.com/wviechtb/status/1228327958810648576/photo/1]

---
### Researcher "degrees of freedom" are difficult to control
.footnote[ Source: Gary King]

.pull-left[
### How are analyses conducted?
- collect the data over many months.
- finish recording and merging.
- run *one* regression.
- new regression, different controls.
- now a different functional form.
- new regression, different measures.
- yet another regression on subset.
- have 100 or 1000 estimates.
- 1 or maybe 5 results in the paper.
]

--

.pull-right[
### What's the problem?
- Some result is designated as the ‚Äúcorrect‚Äù one, only *after* looking
at the estimates.

- Is this a true test of a hypothesis or just confirmation bias?

- This is "p-hacking"
]

---
&lt;img src="/Users/samharper/git/bias-med/images/truth-vigilantes-soccer-calls2.png" width="2731" /&gt;

.footnote[ Source: [fivethirtyeight.com](https://projects.fivethirtyeight.com/p-hacking/)]

---
# Let's do some hacking!

## Go to https://projects.fivethirtyeight.com/p-hacking/ and answer this question:

--

.center[
# .orange[**Will the 2022 US midterm elections affect the economy?**]
]

<div class="countdown" id="timer_624a10ee" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
background-image: url(images/hacking-result.png)
background-size: contain

---
background-image: url(images/hacking-result2.png)
background-size: contain

---
## How NHST facilitates non-replication
.footnote[ Lash (2017)]
.left-column[

.red[Study results are sampled from the (---) distribution, but we only see 'statistically significant' ones ]

]
.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/lash-aje-2017d.png" width="90%" /&gt;
]]


---
.left-column[
.footnote[ https://www.ahajournals.org/doi/abs/10.1161/jaha.116.004880]
### How do we know there is p-hacking?
(1) Look at what people are doing.
]

.right-column[
Two estimates:
- HR=0.90, 95%CI: 0.81, 0.99    .blue["Significantly lower"]
- HR=0.89, 95%CI: 0.78, 1.00009 .red["No difference"]

&lt;img src="/Users/samharper/git/bias-med/images/aha-ns.jpeg" width="700" /&gt;
]
---
.left-column[
.footnote[ Chavalarais et al. (2013)]
### How do we know there is p-hacking?
(2) Seriously, everything is significant
]

.right-column[
P-values in the biomedical literature, 1990-2015
&lt;img src="/Users/samharper/git/bias-med/images/chavalarais-fig3.png" width="700" /&gt;
]

---
.left-column[
.footnote[ Gotzsche (2006)]
### How do we know there is p-hacking?
(3) Maldistribution of published p-values

True for medicine, economics, psychology, political science, many other disciplines.
]

.right-column[
P-values from 260 RCTs
&lt;img src="/Users/samharper/git/bias-med/images/gotzsche.png" width="700" /&gt;
]

---
.left-column[
.footnote[ data from Barnett and Wren [(2019)](https://bmjopen.bmj.com/content/bmjopen/9/11/e032506.full.pdf)]
### Won't 95% confidence intervals help?
No.

Researchers still dichotomize them.
]

.right-column[
.center[Nearly 1,000,000 95% CIs from PubMed:]
&lt;img src="/Users/samharper/git/bias-med/images/wren-cis.png" width="700" /&gt;
]

---
class: middle, center
# NHST also leads to missing evidence and publication bias

---
.footnote[ Turner et al. *NEJM* [(2008)](https://www.nejm.org/doi/full/10.1056/nejmsa065779)]

.left-column[
###  Missing evidence
Negative studies of antidepressents less likely to be published. 

Impacts regulatory decisions.
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/turner-nejm-title.png" width="80%" /&gt;
&lt;img src="/Users/samharper/git/bias-med/images/turner-nejm-figure.png" width="50%" /&gt;
]]

---
.footnote[Fanelli *PLoS ONE* (2010), Yong *Nature* [(2012)](https://www.nature.com/news/replication-studies-bad-copy-1.10634)]

.pull-left[
## .orange[Publication bias affects nearly all disciplines]
### Statistically significant results are more likely to be published, across virtually all disciplines.
### May be worse in "softer" sciences.
### Much of the bias is likely self-imposed.
]

.pull-right[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/yong-accentuate.png" width="90%" /&gt;
]]

---
.footnote[ Figure from Mervis in Science 29 Aug 2014;345:992]

.left-column[
### Self-imposed by many researchers
221 survey experiments funded by US NSF.

All peer reviewed, required to be deposited in a registry.

All studies had results.
]

--

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/mervis-franco-fig.png" width="65%" /&gt;
]]

---
class: center, top, inverse
# .orange[**1. Scientific Integrity Problems**]

.left[
## .gray[**1.1 Mertonian norms**]
## .gray[**1.2 Significance testing**]
## .orange[**1.3 Non-replication**]
## .gray[**1.4 Incentive structure**]
]

---
## Distinctions between commonly used terms
.footnote[ National Academy of Sciences (2019)]
.pull-left[
### Replication
Using using independent investigators, methods, data, equipment, and protocols, we arrive at the same conclusions and/or the same estimate of the effect.

.blue[There can be good reasons why findings do not replicate.]

]

--

.pull-right[
### Reproducibility
If we start from the *same* data gathered by the scientist we can reproduce the same results, p-values, confidence intervals, tables and figures as in the original report.

.red[There are fewer reasons for non-reproducibility.]

]
---
### Large scale efforts to replicate studies are not reassuring
.footnote[ Nosek et al. (2017), Camerer et al. (2016)]

.pull-left[
#### In Psychology
&lt;img src="/Users/samharper/git/bias-med/images/nosek-abstract.png" width="650" /&gt;
]

.pull-right[
#### In Economics
&lt;img src="/Users/samharper/git/bias-med/images/camerer-abstract.png" width="650" /&gt;
]

---
.footnote[ Nosek et al. (2017)]

.left-column[
### Effect sizes are much lower in replication studies.
]
.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/nosek-fig.png" width="650" /&gt;
]

---
## Surely the "top" journals are better, right?
.footnote[ Camerer et al. [(2018)](https://www.nature.com/articles/s41562-018-0399-z)]

.pull-left[
"We find a significant effect in the same direction as the original study for 13 (62%) studies, and the effect size of the replications is on average about 50% of the original effect size"

"The relative effect size of true positives is estimated to be 71%, suggesting that both .red[false positives and inflated effect sizes] of true positives contribute to imperfect reproducibility."
]
.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/camerer-nature-fig-a.png" width="1240" /&gt;
]

---
# What about peer review?
.pull-left[
### Peer review is:
- Slow, inefficient, and expensive.

- Reviewers agreement no better than chance.

- Does not detect errors.
]

.pull-right[
### Reviewiers are biased against:
- Less prestigious institutions.

- Against new or original ideas.
]

.footnote[ Smith (2010), editor at *BMJ* for many years.]


---
### If we wanted to reproduce, often the materials aren't there
.center[
&lt;img src="/Users/samharper/git/bias-med/images/mol-brain.png" width="700" /&gt;
]

.footnote[ Miyakawa *Molecular Brain* (2020) 13:24]

---
.pull-left[
### Even with data, efforts to reproduce are &lt;/br&gt; rarely successful
Gertler et al. gathered replication materials from published papers in econ.

Most authors only included estimation code.

*Estimation code* only ran in 40% of cases.


]

.pull-right[
![](/Users/samharper/git/bias-med/images/gertler-replication.jpg)&lt;!-- --&gt;
]

.footnote[ Gertler et al. 2018]

---
class: center, top, inverse
# .orange[**Outline**]


.left[
## .gray[**What is the replication crisis?**]
## .gray[**What do we mean by replication?**]
## .orange[**What are some potential solutions?**]
]

---
# What is study preregistration?
.left-column[
### A detailed study proposal that is:
]

.right-column[
### Time stamped
#### Records and publicizes time and date.


### Read-only
#### Can't be modified.

### Registered prior to data collection/access
#### Robust to fieldwork, data snooping.
]

---
# What is preregistration?
.left-column[
&lt;img src="/Users/samharper/git/bias-med/images/preregistered_large_color.png" width="100%" height="100%" /&gt;
]

.right-column[
Common / required for publishing most RCTs

Controversial for observational studies.

Idea is to help *reduce publication bias*, since registered studies may be followed over time.

No guarantee anyone will publish.

Also can provide intellectual provenance of your ideas and hypotheses.

Good for planning and hypothesizing, .red[not a straightjacket.]
]

---
# Why preregistration?
.right-column[
## 1. It's *not* about minimizing Type 1 errors.

## 2. It *is* about:
 ###  .blue[Allowing others to transparently evaluate the credibility of the analysis.]
 ###  .blue[Assuring that all of the evidence is available for synthesis.]
]


---
# Writing up pre-registered studies

## 1. Include a link to the registration 
## 2. Report *all* pre-registered results.
## 3. Explain and justify deviations.
## 4. Non-registered analyses appropriately described as "exploratory" or "hypothesis generating".

---
# Why does preregistration matter?

## .orange[Evidence synthesis should be on *all* the evidence.]
## .orange[Distorts planning of future studies.]
## .orange[Unethical and wasteful.]

---
class: center, top, inverse
# .background[**X**]
# .orange[**Registration is useful**]

---
.footnote[redrawn from Kaplan and Irwin [(2015)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0132382)]

.left-column[
In 2000 NHLBI required the registration of primary outcome on ClinicalTrials.gov for all their grant-funded activity.
]

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/kaplan-redrawn.png" width="2301" /&gt;
]


---
## Emphasis on design: .red[Registered Reports]

&lt;img src="/Users/samharper/git/bias-med/images/lee-rr-stage1.png" width="2267" /&gt;

.footnote[ Lee (2019)]

---
## Emphasis on design: .red[Registered Reports]

&lt;img src="/Users/samharper/git/bias-med/images/lee-rr.png" width="2267" /&gt;

.footnote[ Lee (2019)]

---
.footnote[Allen &amp; Mehler, *PLoS Biology* [(2019)](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000246)]

.left-column[
### RRs in Psychology

Little difference between 'replication' studies and 'novel' studies.

Big difference from non-registered studies.
]


.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/allen-rr-2019.png" width="80%" /&gt;
]]

---
class: center, top, inverse
# .background[**X**]
# .orange[**Registration is useful**]
# .orange[**but not sufficient**]

---
.footnote[Zarin *NEJM* (2019)]
.center[
A majority of registerd RCTs still not reported.
&lt;img src="/Users/samharper/git/bias-med/images/zarin-nejm-2019-fig.png" width="70%" /&gt;
]


---
.footnote[ Goldacre [(2019)](https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-019-3173-2)]

.left-column[
### But is preregistration enough?

- Still many differences between registration and published reports.
]

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/goldacre-compare-title.png" width="700" /&gt;
]

---
.footnote[ Goldacre [(2019)](https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-019-3173-2)]

# Academic journals are not helping
.center[
&lt;img src="/Users/samharper/git/bias-med/images/goldacre-compare-table.png" width="700" /&gt;
]

---
# Preregistration is not a panacea

.right-column[
### Preregistered `\(\neq\)` correct/sensible/useful
#### Transparency helps, but cannot fix terrible design or methods.

### Post-hoc analysis can be worthwhile
#### Probing surprising results or mechanisms generates knowledge.

### May also lead to 'halo' effects
#### Preregistered research deserves equal opportunity interrogation.
]

.footnote[See Nosek et al. [(2019)](https://doi.org/10.1016/j.tics.2019.07.009) and Szollosi et al. [(2020)](https://doi.org/10.1016/j.tics.2019.11.009)]

---
class: center, top, inverse
# .orange[**2. Design Solutions**]

.left[
## .gray[**2.1 Preregistration**]
## .orange[**2.2 Pre-analysis plans**]
## .gray[**2.3 Reporting guidelines**]
]

---
## **H**ypothesizing **A**fter the **R**esults are **K**nown (HARKing)

.left-column[
- Pretending what you found was what you were looking for.

- Easy to "find" theory / biological evidence consistent with results.
]

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/targets.png" width="787" /&gt;
]
.footnote[ *New Yorker*, 2014-12-07]

---
# What is a pre-analysis plan?
.left-column[
&lt;img src="/Users/samharper/git/bias-med/images/notebook.png" width="683" /&gt;
]

.right-column[
- Detailed description of research design and data analysis plans, submitted to a registry before looking at the data.

- Helps to tie your hands for data analysis (address researcher degrees of freedom, etc.).

- Distinguish between confirmatory and exploratory analysis.

- Increases the credibility of research.

- Transparent methods make it easier for others to build on your work.
]

---
## Confirmatory and exploratory studies have different aims
.footnote[ Dehaven and Bowman OSF (2020)]
.pull-left[
### Confirmatory
- Well-theorized.

- Plausible mechanisms.

- Minimize false positives.

- Hypothesis *testing*.
]

.pull-right[
### Exploratory
- Pushes new ideas.

- Hypothesis *generating*

- Minimize false negatives.

- Testing irrelevant.
]

---
## What goes into a pre-analysis plan?

.pull-left[
- General info (Title, PIs, Staff)

- Introduction and Summary

- Study Design:
  - Hypotheses
  - Main variables
  - Study setting.
  - Intervention components.
  - Data collection methods.
  - Treatment assignment mechanism.
  - Power calculations.
]

.pull-right[
- Analytic decisions
  - models
  - derived variables
  - clustering
  - multiple testing
  
- Threats/mitigation/robustness checks.

- Dissemination plans
]

---
# Example from development economics
.footnote[ Casey et al. [(2012)](https://academic.oup.com/qje/article-abstract/127/4/1755/1841616?redirectedFrom=fulltext)]

.pull-left[
&lt;img src="/Users/samharper/git/bias-med/images/casey-title.png" width="1549" /&gt;
]

--

.pull-right[
### Conclusions:
&lt;img src="/Users/samharper/git/bias-med/images/casey-conclusion.png" width="1549" /&gt;
]

---
## Example from epidemiology
Note the time-stamp, which provides credible evidence of *when* you had your brilliant ideas.

&lt;img src="/Users/samharper/git/bias-med/images/harper-pap-stamp.png" width="3595" /&gt;


---
.left-column[
### Example from epidemiology
Can be challenging for observational studies or secondary data analyses.

Can you prove when you obtained data access?
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/harper-pap-abstract.png" width="2136" /&gt;
]]

---
class: center, top, inverse
# .orange[**2. Design Solutions**]

.left[
## .gray[**2.1 Preregistration**]
## .gray[**2.2 Pre-analysis plans**]
## .orange[**2.3 Reporting guidelines**]
]

---
.footnote[ Glasziou et al. (2014)]
.pull-left[
# "Most publications have elements that are missing, poorly reported, or ambiguous"
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/glasziou-title.png" width="1797" /&gt;&lt;img src="/Users/samharper/git/bias-med/images/glasziou-table.png" width="1037" /&gt;
]

---
## Importance of intervention details
.footnote[Hoffman (2013)]
.left-column[
Want decision-makers to act on your evidence?

Can they actually understand what you did?
]

--

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/hoffman-title.png" width="2448" /&gt;
- Of 137 interventions, only 53 (39%) were adequately described;

- The most frequently missing item was the ‚Äúintervention materials‚Äù
(47% complete);

]

---
.footnote[Hoffman (2014)]
.left-column[
Missing due to:
- copyright or intellectual property;

- absent materials or intervention details;

- unaware of their importance.
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/hoffman-fig.png" width="80%" /&gt;
]]

---
Reporting guidelines exist for entire research lifecycle

.footnote[See the [EQUATOR Network](https://www.equator-network.org/)]

.pull-left[
.right[
### Question and approach
#### .white[x]
### Pre-intervention
#### .white[x]
### Research report
#### .white[x]
### Cost-effectivness
]]

--

.pull-right[
.left[
### Systematic review
#### üëâ PRISMA/PROSPERO
### Research protocol/preanalysis
#### üëâ SPIRIT
### Trials/Observational studies
#### üëâ CONSORT/STROBE
### Benefits and costs of interventions
#### üëâ CHEERS
]]

---
&lt;img src="/Users/samharper/git/bias-med/images/equator-network-2020.png" width="90%" /&gt;

---
## (Some) evidence that it might matter.
.footnote[Hopewell et al. *BMJ* [(2010)](https://www.bmj.com/content/bmj/340/bmj.c723.full.pdf)]

.left-column[
- Some evidence that item reporting has increased.

- Consistent with revised CONSORT (2001).

- Non-adopting journals report fewer items.
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/hopewell-title.png" width="80%" /&gt;&lt;img src="/Users/samharper/git/bias-med/images/hopewell-bmj-fig2.png" width="80%" /&gt;
]]

---
Since 2015 funders, journals are embracing *Transparency and Openness* (TOP) guidelines.
.center[
&lt;img src="/Users/samharper/git/bias-med/images/TOP_standards.png" width="80%" /&gt;
]

.footnote[Source: OSF]

---
## It's still difficult to change norms
.footnote[Hamra, Goldstein, Harper [(2019)](https://www.ahajournals.org/doi/10.1161/JAHA.119.012292)]

.pull-left[
Most journals chose *Level 1* (disclosure)

*J Am Heart Assoc* published 40 original research papers during first half of 2019.
- Posted data: 0
- Posted code: 1
- Data upon "reasonable" request: 30
- Code upon "reasonable" request: 5


]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/hamra-paper.png" width="2728" /&gt;
]

---
# Value of reporting guidelines

.left-column[
&lt;img src="bias-med_files/figure-html/unnamed-chunk-59-1.png" width="100%" height="100%" /&gt;
]

.right-column[
### Improve transparency of reported research
#### Benefits funders, producers and consumers of research.

### May help to improve the quality of research.
#### More evidence needed, unintented consequences possible.

### Better reporting `\(\neq\)` more reliable.
#### Tranparently reported research can still be biased/bad.
]

---
class: center, middle, inverse
# .orange[**Registration, pre-analysis plans, and reporting guides are design strategies to help mitigate bias from underreported research**]

---
class: center, middle, inverse

## .orange[**They do not guarantee reliable or valid research**]

---

---
class: center, top, inverse
# .orange[**1. Scientific Integrity Problems**]

.left[
## .gray[**1.1 Mertonian norms**]
## .gray[**1.2 Significance testing**]
## .gray[**1.3 Non-replication**]
## .orange[**1.4 Incentive structure**]
]

---
## Incentive problems
.right-column[
### Reward structure
#### Papers, grants, media, "novel" and "significant" results.
### Incentives
#### Gift authorship, CV padding, salami-slicing
#### Overstating claims, ignoring "non-significant" results, p-hacking
#### Hoarding data, non-transparent materials and methods
]

---
## Incentive problems
### Remember Brian Wansink?
### After encouraging his postdoc to "find" specific results, fish for interactions, change the dependent variable, and eliminate outliers, he concluded:

.center[
&lt;img src="/Users/samharper/git/bias-med/images/wansink-careerism.png" width="1877" /&gt;
]

---
# Summary points
## Science is conducted by humans.
## Many counternorms exist that undermine scientific integrity.

--

.center[
## .orange[What can we do about it?]
]

---
### A reproducible path forward: Reminaging the research lifecycle?
&lt;img src="/Users/samharper/git/bias-med/images/research-lifecycle-solution.png" width="1865" /&gt;

.footnote[ Policy Design &amp; Evaluation Lab (2017)]

---
class: center, top, inverse
# .orange[**Break!**] ‚òï
<div class="countdown" id="timer_624a10e3" style="top:15%;right:0;bottom:15%;left:0;margin:5%;padding:50px;font-size:7em;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">10</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
