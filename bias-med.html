<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>The Crisis in Health Research</title>
    <meta charset="utf-8" />
    <meta name="author" content="Sam Harper, Nicholas B. King" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/countdown/countdown.css" rel="stylesheet" />
    <script src="libs/countdown/countdown.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# The Crisis in Health Research
### Sam Harper, Nicholas B. King
### <br> </br>
### <br> </br> Medicine &amp; Society Elective <br> 2022-04-04 </br>

---







.pull-left[
&lt;img src="/Users/samharper/git/bias-med/images/booze-90s.png" width="1055" /&gt;
]

--

.pull-right[
## Questions for you
### 1. When you read or hear about new research (publication or talk), what are things that would make you trust the result? What would make you skeptical?

### 2. Did you ever investigate it further? What did you do, and how did it go?
]

---
class: center, top, inverse
# .orange[**Outline**]

.left[
## .orange[**What is the replication crisis?**]
## .orange[**What caused the crisis?**]
## .orange[**What are some potential solutions?**]
]

---
class: center, top, inverse
# .orange[**Outline**]

.left[
## .orange[**What is the replication crisis?**]
## .gray[**What caused the crisis?**]
## .gray[**What are some potential solutions?**]

]

---
.footnote[Source: Borgman (1997)]
.center[
&lt;img src="/Users/samharper/git/bias-med/images/random.png" width="800" /&gt;
]

---
## In theory...
.footnote[See Ioannidis JPA, [2005](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)]
.pull-left[
Most studies have low prior probability of being true.

Published research is less likely true if:
- Smaller size study.
- Smaller "true" effects.
- Many tests conducted.
- Flexible study design analysis.
- Conflicts of interest present.
- Sexy/timely/popular topic.
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/jpai-plos-2005-title.png" width="80%" /&gt;
]

---
.footnote[Source: [The Economist](https://www.economist.com/science-and-technology/2005/09/01/and-statistics)]
.left-column[

If 10% of hypotheses are true...

AND...we have 80% power...

AND...we only publish 'significant' (*p*&lt;0.05) findings...

...What is the chance that a published positive finding is actually true? 
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/economist.png" width="80%" /&gt;
]]

---
.footnote[Source: [The Economist](https://www.economist.com/science-and-technology/2005/09/01/and-statistics)]
.left-column[

If 10% of hypotheses are true...

AND...we have 80% power...

AND...we only publish 'significant' (*p*&lt;0.05) findings...

...What is the chance that a published positive finding is actually true? 
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/jpai-plos-table.png" width="100%" /&gt;
.white[x]  
.red[Suggests that many published results may be false.]
]]

---
.left-column[
### 2011: Early doubts about ESP in the Psych Lab
&lt;img src="/Users/samharper/git/bias-med/images/esp.png" width="803" /&gt;
]

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/bem.png" width="1356" /&gt;
- Bem (2011) claimed to have found evidence for ESP.
- Two studies that attempted to replicate the Bem study were rejected without peer review at the same journal.

&gt;“This journal does not publish replication studies, whether successful
or unsuccessful...I certainly agree that it’s desirable that replications
are published. The question is where. There are hundreds of journals
in psychology...We don’t want to be the Journal of Bem Replication.”

&gt; -JPSP editor Eliot Smith, as told to the New Scientist.
]

---
.footnote[Source: Science, [2012](https://www.science.org/doi/10.1126/science.338.6112.1270)]

.pull-left[
### 2011: Fraud!
- Psychologist Diederik Stapel admits to fabricating data for many psychology studies.

- 58 papers retracted.

- Concerns raised about reliability of evidence for the entire field of psychology.
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/stapel-science.png" width="1547" /&gt;
]

---
.footnote[Source: Pashler and Wagenmakers [2012](https://journals.sagepub.com/doi/pdf/10.1177/1745691612465253)]

.pull-left[
### Systemic concerns
- Fraud and ESP studies

- Psychologists unwilling to share data or code.

- Admission of Questionable Research Practices (QRPs).

- Evidence of manipulating results to get *p*&lt;0.05.

- Well-known results unable to be replicated.
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/pashler-psychsci-2012.png" width="1837" /&gt;
]

---
### Meanwhile, in biomedical research...
.footnote[Source: C Glenn Begley and Lee M Ellis, “Drug development: Raise standards for preclinical cancer research”, *Nature*, 483(7391), 2012, pp. 531–3.]

.pull-left[
Authors tried to replicate 53 “landmark” drug development studies.

Only 6 (11%) successfully replicated. 

Why not?  
- Non blinding of experimental vs. control groups. 

- Selective reporting of research results. 
]

.pull-right[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/nature-rep.png" width="907" /&gt;
]]

---
### Non-replication of candidate gene studies
.footnote[Ioannidis (2011)]
.center[
&lt;img src="/Users/samharper/git/bias-med/images/ioannidis-epid.png" width="80%" /&gt;
]

---
.footnote[See Ritchie, *Science Fictions* (2019), Baker *Nature* (2018)]
.pull-left[
## Et cetera, et cetera...
### Also evidence of high-profile non-replication studies in:
- Psychiatry
- Neurobiology
- Chemistry
- Biology
- Ecology &amp; evolution
- Oceanography
- Likely any place we have not looked...
]

.pull-right[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/baker-cover.png" width="80%" /&gt;
]]

---
class: center, top, inverse
# .background[x]

# .orange[Concerns about reliablility of findings across multiple disciplines led to collaborative, large-scale *replication* studies.]

---
## Distinctions between commonly used terms
.footnote[ National Academy of Sciences (2019)]
.pull-left[
### Replication
Using using independent investigators, methods, data, equipment, and protocols, we arrive at the same conclusions and/or the same estimate of the effect.

.blue[There can be good reasons why findings do not replicate.]

]

--

.pull-right[
### Reproducibility
If we start from the *same* data gathered by the scientist we can reproduce the same results, p-values, confidence intervals, tables and figures as in the original report.

.red[There are fewer reasons for non-reproducibility.]

]

---
.footnote[Source: https://the-turing-way.netlify.app/reproducible-research/overview/overview-definitions.html]

.left-column[
## Definitions
- May not be used consistently.

- Clarify whether data and analysis are different from original study.
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/rr-matrix.png" width="80%" /&gt;
]]

---
.footnote[Nosek et al., [2015](https://www-science-org/doi/10.1126/science.aac4716). Also see letters and responses: http://bit.ly/1Xg9lQl]

.pull-left[
&lt;img src="/Users/samharper/git/bias-med/images/osc-2015-method.png" width="1011" /&gt;
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/nosek-abstract.png" width="1263" /&gt;
]

---
.footnote[ Nosek et al. (2015)]

.left-column[
### Effect sizes are much lower in replication studies.
]
.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/nosek-fig.png" width="600" /&gt;
]

---
.footnote[Camerer et al. (2016)]

.left-column[
### No better in Economics
- Increased the number of participants by a factor of five

- Preregistered their study and analysis designs before any data collection.
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/camerer-abstract.png" width="650" /&gt;
]]

---
## Surely the "top" journals are better, right?
.footnote[ Camerer et al. [(2018)](https://www.nature.com/articles/s41562-018-0399-z)]

.pull-left[
"We find a significant effect in the same direction as the original study for 13 (62%) studies, and the effect size of the replications is on average about 50% of the original effect size"

"The relative effect size of true positives is estimated to be 71%, suggesting that both .red[false positives and inflated effect sizes] of true positives contribute to imperfect reproducibility."
]
.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/camerer-nature-fig-a.png" width="1240" /&gt;
]

---
## What is a replication?
.footnote[See Nosek and Errington, PLoS Biology [2020](https://www.science.org/doi/10.1126/science.338.6112.1270). GIF from [tenor.com](https://tenor.com/view/spider-man-we-one-gif-18212100.gif).]

.pull-left[
- *Exact* replication is impossible (without time travel)

- New studies involve new subjects, different environment.

- Should not expect exactly the same results.

- Better to think of it as a study that can tell us something about the veracity of a prior claim.
]

.pull-right[
.center[
![](https://tenor.com/view/spider-man-we-one-gif-18212100.gif)]]

---
.footnote[Source: Evans et al., *Testing Treatments* (2011)]
.left-column[
### Can think of cluster-randomized trials as replications
]
.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/crash2-meta-analysis.png" width="80%" /&gt;
]]

---
## Do we really mean 'replication'?

.center[
&lt;img src="/Users/samharper/git/bias-med/images/childers-title.png" width="90%" /&gt;
]

---
## Do we really mean 'replication'?

.center[
&lt;img src="/Users/samharper/git/bias-med/images/childers-title-rev.png" width="90%" /&gt;
]


---
### Considering 'robustness'

.footnote[Shanil Ebrahim, et al., “Reanalyses of randomized clinical trial data”, *JAMA*, 312(10), 2014, pp. 1024–32.]

.left-column[
- What if we tweak prior methods?

- Many decisions affect the 'final' result.

- Would it matter?
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/ebrahim-title.png" width="90%" /&gt;
]]

---
### What leads to different answers?
- Reducing measurement error:
.center[
&lt;img src="/Users/samharper/git/bias-med/images/ebrahim1.png" width="80%" /&gt;
]
- Allowing for effect modification:
.center[
&lt;img src="/Users/samharper/git/bias-med/images/ebrahim2.png" width="80%" /&gt;
]

---
.footnote[Baker *Nature* (2018)]
.left-column[
## Scentists think there is a "reproducibility" crisis

### or a "slight" crisis?  🤔
]
.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/baker-crisis.png" width="70%" /&gt;
]]

---
class: center, top, inverse
# .orange[**Outline**]

.left[
## .gray[**What is the replication crisis?**]
## .orange[**What caused the crisis?**]
## .gray[**What are some potential solutions?**]
]


---
## Potential sources of "bias" in published research
.pull-left[
### Usual explanations
#### Confounding, measurement error, selection bias, model misspecification, etc.
]

--

.pull-right[
### Problems with integrity
#### • Fraud/data manipulation/fabrication.
#### • Poor design / inadequate power.
#### • NHST: Publication bias.
#### • NHST: P-hacking.
#### • Financial ties/ideological commitments.
#### • Careerism.
#### • Lack of transparency.
]

---
.footnote[ George, Int J Clin Oncol (2016) 21:15–21]

.center[
&lt;img src="/Users/samharper/git/bias-med/images/george-fraud-table.png" width="100%" /&gt;
]

---
### The coronavirus pandemic has not helped.
.footnote[See reporting by Davey et al. in [*The Guardian*](https://www.theguardian.com/world/2020/jun/03/covid-19-surgisphere-who-world-health-organization-hydroxychloroquine)]

.pull-left[
- High profile studies of 96k patients across 671 hospitals.

- Claimed hydroxychloroquine increased mortality from COVID-19.

- Immediately led to WHO halting the hydroxychloroquine arm of its global trials.

- Researchers subsequently questioned irregularities in the data, ethics review, protocols, statistical analysis. 

- Two papers (other in *NEJM*) were retracted.
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/surgisphere-lancet-retracted.png" width="100%" /&gt;
]

---
### The coronavirus pandemic has not helped.
.footnote[See reporting by [Meyerowitz-Katz](https://gidmk.medium.com/is-ivermectin-for-covid-19-based-on-fraudulent-research-part-4-f30eeb30d2ff)]

.pull-left[
- Ivermectin known to be effective for parasitic diseases.

- Lebanese trial claimed *huge effect* of ivermectin on lowering SARS-CoV2 viral load.

- Requests for trial data to verify the results by independent researchers were denied.

- Researchers subsequently gained access, found serious irregularities. 

- Ultimately retracted.
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/ivermectin-trial-data.png" width="100%" /&gt;
]

---
.footnote[Reis et al. *NEJM*, March 30, 2022 DOI: 10.1056/NEJMoa2115869]

.center[
&lt;img src="/Users/samharper/git/bias-med/images/reis-nejm-title.png" width="80%" /&gt;&lt;img src="/Users/samharper/git/bias-med/images/reis-nejm-table2.png" width="80%" /&gt;
]

---
.pull-left[
## A lot of irreproducible or unreliable research stems from Null Hypothesis Significance Testing (NHST).
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/pushing-for-p.png" width="2435" /&gt;
]

.footnote[ https://mobile.twitter.com/wviechtb/status/1228327958810648576/photo/1]

---
.footnote[ Lash (2017)]
.left-column[
### NHST facilitates non-replication
.red[Study results are sampled from the (---) distribution, but we only see 'statistically significant' ones ]

]
.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/lash-aje-2017d.png" width="90%" /&gt;
]]

--
Implications for planning, meta-analysis, and replication studies.

---
### The coronavirus pandemic has not helped.
.footnote[Source: Pearson, *Nature* (2021)]

.pull-left[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/pearson-samples.png" width="80%" /&gt;
]]

.pull-right[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/pearson-trials.png" width="70%" /&gt;
]]

---
### Researcher "degrees of freedom" are difficult to control
.footnote[ Source: Gary King]

.pull-left[
### How are analyses conducted?
- collect the data over many months.
- finish recording and merging.
- run *one* regression.
- new regression, different controls.
- now a different functional form.
- new regression, different measures.
- yet another regression on subset.
- have 100 or 1000 estimates.
- 1 or maybe 5 results in the paper.
]

--

.pull-right[
### What's the problem?
- Some result is designated as the “correct” one, only *after* looking
at the estimates.

- Is this a true test of a hypothesis or just confirmation bias?

- This is "p-hacking"
]

---
&lt;img src="/Users/samharper/git/bias-med/images/truth-vigilantes-soccer-calls2.png" width="2731" /&gt;

.footnote[ Source: [fivethirtyeight.com](https://projects.fivethirtyeight.com/p-hacking/)]

---
# Let's do some hacking!

## Go to https://projects.fivethirtyeight.com/p-hacking/ and answer this question:

--

.center[
# .orange[**Will the 2022 US midterm elections affect the economy?**]
]

<div class="countdown" id="timer_624afd6a" style="right:0;bottom:0;" data-warnwhen="0">
<code class="countdown-time"><span class="countdown-digits minutes">03</span><span class="countdown-digits colon">:</span><span class="countdown-digits seconds">00</span></code>
</div>

---
background-image: url(images/hacking-result.png)
background-size: contain

---
background-image: url(images/hacking-result2.png)
background-size: contain


---
.left-column[
.footnote[ https://www.ahajournals.org/doi/abs/10.1161/jaha.116.004880]
### How do we know there is p-hacking?
(1) Look at what people are doing.
]

.right-column[
Two estimates:
- HR=0.90, 95%CI: 0.81, 0.99    .blue["Significantly lower"]
- HR=0.89, 95%CI: 0.78, 1.00009 .red["No difference"]

&lt;img src="/Users/samharper/git/bias-med/images/aha-ns.jpeg" width="700" /&gt;
]

---
### Good advice on how to get *p*&lt;0.05

.pull-left[
&lt;img src="/Users/samharper/git/bias-med/images/wansink-manipulation.png" width="1877" /&gt;&lt;img src="/Users/samharper/git/bias-med/images/wansink-p-hacking-dv.png" width="1877" /&gt;
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/wansink-p-hacking-int.png" width="1877" /&gt;
]

---
.footnote[Archived post [here](https://web.archive.org/web/20170312041524/http:/www.brianwansink.com/phd-advice/the-grad-student-who-never-said-no).]

.pull-left[
*"I gave her a data set of a self-funded, failed study which had .red[null results]... I said, ‘This cost us a lot of time and our own money to collect. There’s got to be something here we can salvage because it’s a cool (rich &amp; unique) data set.’ I had three ideas for potential Plan B, C, &amp; D directions (since Plan A had failed)."* -blog, 2016

Enterprising grad students found:
- impossible values
- incorrect ANOVA results
- dubious p-values

Wansink denied requests for access to the original data.
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/wansink-vox.png" width="2312" /&gt;

Wansink resigned from Cornell in 2019.
]


---
.left-column[
.footnote[ Chavalarais et al. (2013)]
### How do we know there is p-hacking?
(2) Seriously, everything is significant
]

.right-column[
P-values in the biomedical literature, 1990-2015
&lt;img src="/Users/samharper/git/bias-med/images/chavalarais-fig3.png" width="700" /&gt;
]

---
.left-column[
.footnote[ Gotzsche (2006)]
### How do we know there is p-hacking?
(3) Maldistribution of published p-values

True for medicine, economics, psychology, political science, many other disciplines.
]

.right-column[
P-values from 260 RCTs
&lt;img src="/Users/samharper/git/bias-med/images/gotzsche.png" width="700" /&gt;
]

---
.left-column[
.footnote[ data from Barnett and Wren [(2019)](https://bmjopen.bmj.com/content/bmjopen/9/11/e032506.full.pdf)]
### Won't 95% confidence intervals help?
No.

Researchers still dichotomize them.
]

.right-column[
.center[Nearly 1,000,000 95% CIs from PubMed:]
&lt;img src="/Users/samharper/git/bias-med/images/wren-cis.png" width="700" /&gt;
]

---
class: middle, center
# NHST also leads to missing evidence and publication bias

---
.footnote[ Turner et al. *NEJM* [(2008)](https://www.nejm.org/doi/full/10.1056/nejmsa065779)]

.left-column[
###  Missing evidence
Negative studies of antidepressents less likely to be published. 

Impacts regulatory decisions.
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/turner-nejm-title.png" width="80%" /&gt;
&lt;img src="/Users/samharper/git/bias-med/images/turner-nejm-figure.png" width="50%" /&gt;
]]

---
.footnote[Fanelli *PLoS ONE* (2010), Yong *Nature* [(2012)](https://www.nature.com/news/replication-studies-bad-copy-1.10634)]

.pull-left[
## .orange[Publication bias affects nearly all disciplines]
### Statistically significant results are more likely to be published, across virtually all disciplines.
### May be worse in "softer" sciences.
### Much of the bias is likely self-imposed.
]

.pull-right[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/yong-accentuate.png" width="90%" /&gt;
]]

---
.footnote[ Figure from Mervis in Science 29 Aug 2014;345:992]

.left-column[
### Self-imposed by many researchers
221 survey experiments funded by US NSF.

All peer reviewed, required to be deposited in a registry.

All studies had results.
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/mervis-franco-fig.png" width="65%" /&gt;
]]

---
# What about peer review?
.pull-left[
### Peer review is:
- Slow, inefficient, and expensive.

- Reviewers agreement no better than chance.

- Does not detect errors.
]

.pull-right[
### Reviewiers are biased against:
- Less prestigious institutions.

- Against new or original ideas.
]

.footnote[ Smith (2010), editor at *BMJ* for many years.]

---
## Okay, forget peer review
.footnote[See https://www.cbc.ca/news/health/covid-19-vaccine-study-error-anti-vaxxers-1.6188806]
.pull-left[
- Study *preprints* get science out rapidly, before peer review.

- Explosion of preprints during COVID-19

- 12 authors from Ottawa Heart Institute published preprint claiming 1 in 1000 vaccine recipients had myocarditis.
  - Case series
  - Wrong denominators (33,000 instead of 850,000)
  - Revised risk 1 in 25,000
]

.pull-right[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/kafil-withdrawn.png" width="100%" /&gt;
]]


---
### If we wanted to reproduce, often the materials aren't there
.center[
&lt;img src="/Users/samharper/git/bias-med/images/mol-brain.png" width="700" /&gt;
]

.footnote[ Miyakawa *Molecular Brain* (2020) 13:24]

---
.pull-left[
### Even with data, efforts to reproduce are &lt;/br&gt; rarely successful
Gertler et al. gathered replication materials from published papers in econ.

Most authors only included estimation code.

*Estimation code* only ran in 40% of cases.


]

.pull-right[
![](/Users/samharper/git/bias-med/images/gertler-replication.jpg)&lt;!-- --&gt;
]

.footnote[ Gertler et al. 2018]

---
class: center, top, inverse
# .orange[**Outline**]


.left[
## .gray[**What is the replication crisis?**]
## .gray[**What do we mean by replication?**]
## .orange[**What are some potential solutions?**]
]

---
class: center, middle, inverse

# .orange[**Preregistration of studies**]

---
# What is study preregistration?
.left-column[
### A detailed study proposal that is:
]

.right-column[
### Time stamped
#### Records and publicizes time and date.


### Read-only
#### Can't be modified.

### Registered prior to data collection/access
#### Robust to fieldwork, data snooping.
]

---
# What is preregistration?
.left-column[
&lt;img src="/Users/samharper/git/bias-med/images/preregistered_large_color.png" width="100%" height="100%" /&gt;
]

.right-column[
Common / required for publishing most RCTs

Controversial for observational studies.

Idea is to help *reduce publication bias*, since registered studies may be followed over time.

No guarantee anyone will publish.

Also can provide intellectual provenance of your ideas and hypotheses.

Good for planning and hypothesizing, .red[not a straightjacket.]
]

---
# Why preregistration?
.right-column[
## 1. It's *not* about minimizing Type 1 errors.

## 2. It *is* about:
 ###  .blue[Allowing others to transparently evaluate the credibility of the analysis.]
 ###  .blue[Assuring that all of the evidence is available for synthesis.]
]

---
# Why does preregistration matter?

## .orange[Evidence synthesis should be on *all* the evidence.]
## .orange[Distorts planning of future studies.]
## .orange[Unethical and wasteful.]

---
class: center, top, inverse
# .background[**X**]
# .orange[**Registration is useful**]

---
.footnote[redrawn from Kaplan and Irwin [(2015)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0132382)]

.left-column[
In 2000 NHLBI required the registration of primary outcome on ClinicalTrials.gov for all their grant-funded activity.
]

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/kaplan-redrawn.png" width="2301" /&gt;
]

---
class: center, middle, inverse

# .orange[**What if my results are null?**]

# .orange[**You showed us that they won't get published!**]



---
## Emphasis on design: .red[Registered Reports]

&lt;img src="/Users/samharper/git/bias-med/images/lee-rr-stage1.png" width="2267" /&gt;

.footnote[ Lee (2019)]

---
## Emphasis on design: .red[Registered Reports]

&lt;img src="/Users/samharper/git/bias-med/images/lee-rr.png" width="2267" /&gt;

.footnote[ Lee (2019)]

---
.footnote[Allen &amp; Mehler, *PLoS Biology* [(2019)](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000246)]

.left-column[
### RRs in Psychology

Little difference between 'replication' studies and 'novel' studies.

Big difference from non-registered studies.
]


.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/allen-rr-2019.png" width="80%" /&gt;
]]

---
class: center, top, inverse
# .background[**X**]
# .orange[**Registration is useful**]
# .orange[**but not sufficient**]

---
.footnote[Zarin *NEJM* (2019)]
.center[
A majority of (pre-COVID-19) registered RCTs still not reported.
&lt;img src="/Users/samharper/git/bias-med/images/zarin-nejm-2019-fig.png" width="80%" /&gt;
]


---
.footnote[ Goldacre [(2019)](https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-019-3173-2)]

.left-column[
### But is preregistration enough?

- Still many differences between registration and published reports.
]

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/goldacre-compare-title.png" width="700" /&gt;
]

---
.footnote[ Goldacre [(2019)](https://trialsjournal.biomedcentral.com/articles/10.1186/s13063-019-3173-2)]

# Academic journals are not helping
.center[
&lt;img src="/Users/samharper/git/bias-med/images/goldacre-compare-table.png" width="700" /&gt;
]

---
# Preregistration is not a panacea

.right-column[
### Preregistered `\(\neq\)` correct/sensible/useful
#### Transparency helps, but cannot fix terrible design or methods.

### Post-hoc analysis can be worthwhile
#### Probing surprising results or mechanisms generates knowledge.

### May also lead to 'halo' effects
#### Preregistered research deserves equal opportunity interrogation.
]

.footnote[See Nosek et al. [(2019)](https://doi.org/10.1016/j.tics.2019.07.009) and Szollosi et al. [(2020)](https://doi.org/10.1016/j.tics.2019.11.009)]

---
class: center, middle, inverse

# .orange[**Pre-analysis plans**]


---
## **H**ypothesizing **A**fter the **R**esults are **K**nown (HARKing)
.footnote[ *New Yorker*, 2014-12-07]

.left-column[
- Pretending what you found was what you were looking for.

- Easy to "find" theory / biological evidence consistent with results.
]

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/targets.png" width="787" /&gt;
]


---
# What is a pre-analysis plan?
.left-column[
&lt;img src="/Users/samharper/git/bias-med/images/notebook.png" width="683" /&gt;
]

.right-column[
- Detailed description of research design and data analysis plans, submitted to a registry before looking at the data.

- Helps to tie your hands for data analysis (address researcher degrees of freedom, etc.).

- Distinguish between confirmatory and exploratory analysis.

- Increases the credibility of research.

- Transparent methods make it easier for others to build on your work.
]

---
## Confirmatory and exploratory studies have different aims
.footnote[ Dehaven and Bowman OSF (2020)]
.pull-left[
### Confirmatory
- Well-theorized.

- Plausible mechanisms.

- Minimize false positives.

- Hypothesis *testing*.
]

.pull-right[
### Exploratory
- Pushes new ideas.

- Hypothesis *generating*

- Minimize false negatives.

- Testing irrelevant.
]

---
## What goes into a pre-analysis plan?

.pull-left[
- General info (Title, PIs, Staff)

- Introduction and Summary

- Study Design:
  - Hypotheses
  - Main variables
  - Study setting.
  - Intervention components.
  - Data collection methods.
  - Treatment assignment mechanism.
  - Power calculations.
]

.pull-right[
- Analytic decisions
  - models
  - derived variables
  - clustering
  - multiple testing
  
- Threats/mitigation/robustness checks.

- Dissemination plans
]

---
## Example from epidemiology
Note the time-stamp, which provides credible evidence of *when* you had your brilliant ideas.

&lt;img src="/Users/samharper/git/bias-med/images/harper-pap-stamp.png" width="3595" /&gt;


---
.left-column[
### Example from epidemiology
Can be challenging for observational studies or secondary data analyses.

Can you prove when you obtained data access?
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/harper-pap-abstract.png" width="2136" /&gt;
]]

---
class: center, top, inverse
# .orange[**2. Design Solutions**]

.left[
## .gray[**2.1 Preregistration**]
## .gray[**2.2 Pre-analysis plans**]
## .orange[**2.3 Reporting guidelines**]
]

---
.footnote[ Glasziou et al. (2014)]
.pull-left[
# "Most publications have elements that are missing, poorly reported, or ambiguous"
]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/glasziou-title.png" width="1797" /&gt;&lt;img src="/Users/samharper/git/bias-med/images/glasziou-table.png" width="1037" /&gt;
]

---
## Importance of intervention details
.footnote[Hoffman (2013)]
.left-column[
Want decision-makers to act on your evidence?

Can they actually understand what you did?
]

--

.right-column[
&lt;img src="/Users/samharper/git/bias-med/images/hoffman-title.png" width="2448" /&gt;
- Of 137 interventions, only 53 (39%) were adequately described;

- The most frequently missing item was the “intervention materials”
(47% complete);

]

---
.footnote[Hoffman (2014)]
.left-column[
Missing due to:
- copyright or intellectual property;

- absent materials or intervention details;

- unaware of their importance.
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/hoffman-fig.png" width="80%" /&gt;
]]

---
Reporting guidelines exist for entire research lifecycle

.footnote[See the [EQUATOR Network](https://www.equator-network.org/)]

.pull-left[
.right[
### Question and approach
#### .white[x]
### Pre-intervention
#### .white[x]
### Research report
#### .white[x]
### Cost-effectivness
]]

--

.pull-right[
.left[
### Systematic review
#### 👉 PRISMA/PROSPERO
### Research protocol/preanalysis
#### 👉 SPIRIT
### Trials/Observational studies
#### 👉 CONSORT/STROBE
### Benefits and costs of interventions
#### 👉 CHEERS
]]

---
&lt;img src="/Users/samharper/git/bias-med/images/equator-network-2020.png" width="90%" /&gt;

---
## (Some) evidence that it might matter.
.footnote[Hopewell et al. *BMJ* [(2010)](https://www.bmj.com/content/bmj/340/bmj.c723.full.pdf)]

.left-column[
- Some evidence that item reporting has increased.

- Consistent with revised CONSORT (2001).

- Non-adopting journals report fewer items.
]

.right-column[
.center[
&lt;img src="/Users/samharper/git/bias-med/images/hopewell-title.png" width="80%" /&gt;&lt;img src="/Users/samharper/git/bias-med/images/hopewell-bmj-fig2.png" width="80%" /&gt;
]]

---
Since 2015 funders, journals are embracing *Transparency and Openness* (TOP) guidelines.
.center[
&lt;img src="/Users/samharper/git/bias-med/images/TOP_standards.png" width="80%" /&gt;
]

.footnote[Source: OSF]

---
## It's still difficult to change norms
.footnote[Hamra, Goldstein, Harper [(2019)](https://www.ahajournals.org/doi/10.1161/JAHA.119.012292)]

.pull-left[
Most journals chose *Level 1* (disclosure)

*J Am Heart Assoc* published 40 original research papers during first half of 2019.
- Posted data: 0
- Posted code: 1
- Data upon "reasonable" request: 30
- Code upon "reasonable" request: 5


]

.pull-right[
&lt;img src="/Users/samharper/git/bias-med/images/hamra-paper.png" width="2728" /&gt;
]

---
# Value of reporting guidelines

.left-column[
&lt;img src="bias-med_files/figure-html/unnamed-chunk-70-1.png" width="100%" height="100%" /&gt;
]

.right-column[
### Improve transparency of reported research
#### Benefits funders, producers and consumers of research.

### May help to improve the quality of research.
#### More evidence needed, unintented consequences possible.

### Better reporting `\(\neq\)` more reliable.
#### Tranparently reported research can still be biased/bad.
]

---
class: center, middle, inverse
# .orange[**Registration, pre-analysis plans, and reporting guides are design strategies to help mitigate bias from underreported research**]

---
class: center, middle, inverse

## .orange[**They do not guarantee reliable or valid research**]

---
## Incentive problems
.right-column[
### Reward structure
#### Papers, grants, media, "novel" and "significant" results.
### Incentives
#### Gift authorship, CV padding, salami-slicing
#### Overstating claims, ignoring "non-significant" results, p-hacking
#### Hoarding data, non-transparent materials and methods
]

---
# Summary points
.right-column[
### Science is conducted by humans.
#### Extra-scientific factors matter, not necessarily malicious intent. 

### Changing norms is hard
#### Many incentives exist that undermine scientific integrity.

### Transparency and openness can help
#### Making research process transparent is the bare minimum, and does not guarantee 'true' results.
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
